{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "[2]Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "782pDvqa_Itg"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JangAyeon/ToBigs/blob/master/%5B2%5DOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90grJPRt_ItX"
      },
      "source": [
        "# Tobig's 15기 2주차 Optimization 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEyXE6gV_Itc"
      },
      "source": [
        "# Gradient Descent 구현하기\n",
        "\n",
        "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
        "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422BL0TT_Itd"
      },
      "source": [
        "## 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bPHAVuA_Itd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyyhxhfx_Ite",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "4001f157-75c2-4a69-c533-c3341034e6ca"
      },
      "source": [
        "data = pd.read_csv('assignment_2.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7</td>\n",
              "      <td>48000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>48000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.2</td>\n",
              "      <td>63000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>76000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label  bias  experience  salary\n",
              "0      1     1         0.7   48000\n",
              "1      0     1         1.9   48000\n",
              "2      1     1         2.5   60000\n",
              "3      0     1         4.2   63000\n",
              "4      0     1         6.0   76000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ayYoXQ4_Ite"
      },
      "source": [
        "## Train Test 데이터 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI4GZgfN_Ite"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6QpRYCC_Ite"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIFhpP7I_Itf",
        "outputId": "2efeb787-7c99-4435-c1aa-9ec6a328dfe6"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 3), (50, 3), (150,), (50,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "YDpeZsm2NWuQ",
        "outputId": "7dceb0d7-e669-4de8-a0b7-67dae7ca4b8b"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>1</td>\n",
              "      <td>5.3</td>\n",
              "      <td>48000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>1</td>\n",
              "      <td>8.1</td>\n",
              "      <td>66000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>1</td>\n",
              "      <td>3.9</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>45000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>66000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>64000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>1</td>\n",
              "      <td>4.8</td>\n",
              "      <td>73000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>86000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>7.6</td>\n",
              "      <td>78000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>1</td>\n",
              "      <td>6.3</td>\n",
              "      <td>76000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     bias  experience  salary\n",
              "71      1         5.3   48000\n",
              "124     1         8.1   66000\n",
              "184     1         3.9   60000\n",
              "97      1         0.2   45000\n",
              "149     1         1.1   66000\n",
              "..    ...         ...     ...\n",
              "67      1         6.7   64000\n",
              "192     1         4.8   73000\n",
              "117     1         7.0   86000\n",
              "47      1         7.6   78000\n",
              "172     1         6.3   76000\n",
              "\n",
              "[150 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HoONpPh_Itf"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "Qqlmm5s6_Itf",
        "outputId": "7a9c74ec-f3b4-4bd1-d267-63bea3df2a90"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "bias_train = X_train[\"bias\"]\n",
        "bias_train = bias_train.reset_index()[\"bias\"]\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "X_train[\"bias\"] = bias_train\n",
        "X_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.187893</td>\n",
              "      <td>-1.143335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.185555</td>\n",
              "      <td>0.043974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.310938</td>\n",
              "      <td>-0.351795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.629277</td>\n",
              "      <td>-1.341220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.308600</td>\n",
              "      <td>0.043974</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bias  experience    salary\n",
              "0     1    0.187893 -1.143335\n",
              "1     1    1.185555  0.043974\n",
              "2     1   -0.310938 -0.351795\n",
              "3     1   -1.629277 -1.341220\n",
              "4     1   -1.308600  0.043974"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5idcw4Iw_Itf"
      },
      "source": [
        "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
        "똑같이 X_test에다 fit하면 안돼요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "nFA2nI4g_Itg",
        "outputId": "861cb119-4aec-4933-8ed5-c92a41d31ae3"
      },
      "source": [
        "bias_test = X_test[\"bias\"]\n",
        "bias_test = bias_test.reset_index()[\"bias\"]\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
        "X_test[\"bias\"] = bias_test\n",
        "X_test.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.344231</td>\n",
              "      <td>-0.615642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.508570</td>\n",
              "      <td>0.307821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.310938</td>\n",
              "      <td>0.571667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.363709</td>\n",
              "      <td>1.956862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.987923</td>\n",
              "      <td>-0.747565</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bias  experience    salary\n",
              "0     1   -1.344231 -0.615642\n",
              "1     1    0.508570  0.307821\n",
              "2     1   -0.310938  0.571667\n",
              "3     1    1.363709  1.956862\n",
              "4     1   -0.987923 -0.747565"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5N0yc9K_Itg"
      },
      "source": [
        "# parameter 개수\n",
        "N = len(X_train.loc[0])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR6JjcVr_Itg",
        "outputId": "fb78e41c-9ad1-4ab1-dbcb-733b07af1b0d"
      },
      "source": [
        "# 초기 parameter들을 임의로 설정해줍니다.\n",
        "parameters = np.array([random.random() for i in range(N)])\n",
        "random_parameters = parameters.copy()\n",
        "parameters"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.14301419, 0.40251885, 0.60800072])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782pDvqa_Itg"
      },
      "source": [
        "### * LaTeX   \n",
        "\n",
        "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
        "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
        "http://triki.net/apps/3466  \n",
        "https://jjycjnmath.tistory.com/117"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icE0hEAZ_Ith"
      },
      "source": [
        "## Dot product\n",
        "## $z = X_i \\theta$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KodweEt_Ith"
      },
      "source": [
        "def dot_product(X, parameters):\n",
        "    z = 0\n",
        "    for i in range(len(parameters)):\n",
        "        z +=X[i]*parameters.T[i]\n",
        "    return z"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5EDfifh_Ith"
      },
      "source": [
        "## Logistic Function\n",
        "\n",
        "## $p =\\frac{1}{1+e^{-Z}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNxzHG-S_Ith"
      },
      "source": [
        "def logistic(X, parameters):\n",
        "    z = dot_product(X,parameters)\n",
        "    p =np.exp(z)/(1 + np.exp(z))   \n",
        "    return p"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlV8rkCb_Iti",
        "outputId": "a9a5f628-0105-4589-db70-7e4fd528c334"
      },
      "source": [
        "logistic(X_train.iloc[1], parameters)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6563248503041921"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0yoG93_Iti"
      },
      "source": [
        "## Object function\n",
        "\n",
        "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
        "<br>\n",
        "선형 회귀의 목적함수\n",
        "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
        "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
        "  \n",
        "로지스틱 회귀의 목적함수를 작성해주세요  \n",
        "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
        "## $l(p) = -\\Sigma((y_ilog p(X_i))+(i-y_i)log(1-p(X_i)))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIWmO8w-_Iti"
      },
      "source": [
        "def minus_log_cross_entropy_i(X, y, parameters):\n",
        "    p = logistic(X,parameters)\n",
        "    loss =-np.sum((y*np.log(p)+(1-y)*np.log(1-p)))\n",
        "    return loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trKQWp0__Iti"
      },
      "source": [
        "def mse_i(X, y, parameters):\n",
        "    y_hat = dot_product(X,parameters)\n",
        "    loss = (np.sum((y-y_hat)**2))/2\n",
        "    return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE0mC-qz_Itj"
      },
      "source": [
        "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(X_set.shape[0]):\n",
        "        X = X_set.iloc[i,:]\n",
        "        y = y_set.iloc[i]\n",
        "        loss +=loss_function(X,y,parameters)\n",
        "    loss = loss/n #loss 평균값으로 계산\n",
        "    return loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoYuSq_x_Itj",
        "outputId": "4aa87c62-f2f3-4013-f2e1-875514998da2"
      },
      "source": [
        "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9780374264298508"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1xLcVl7_Itj"
      },
      "source": [
        "## Gradient\n",
        "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
        "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af12kyis_Itj"
      },
      "source": [
        "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=\\Sigma(y_i-\\theta^{T}X_i)X_{ik}$\n",
        "## ${\\partial\\over{\\partial \\theta_j}}l(p)=\\Sigma((y_i-p_i)X_{ij})$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q014mMmH_Itk"
      },
      "source": [
        "def get_gradient_ij(X, y, parameters, j, model):\n",
        "    if model == 'linear':\n",
        "        y_hat = dot_product(parameters.T,X)\n",
        "        gradient =X[j]*(y-y_hat)\n",
        "    else:\n",
        "        p =logistic(X,parameters)\n",
        "        gradient =(y-p) *X[j]\n",
        "    return -gradient"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P74GPqS9_Itk",
        "outputId": "f1c0119c-44cb-4bb4-c4e4-eed64aca2092"
      },
      "source": [
        "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.11591534229568734"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyn-INfX_Itk"
      },
      "source": [
        "## Batch Gradient\n",
        "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8g7qzPM_Itk"
      },
      "source": [
        "def batch_gradient(X_set, y_set, parameters, model):\n",
        "    gradients = [0 for _ in range(len(parameters))]\n",
        "    \n",
        "    for i in range(X_set.shape[0]):\n",
        "        X = X_set.iloc[i,:]\n",
        "        y = y_set.iloc[i]\n",
        "        for j in range(len(parameters)):\n",
        "            gradients[j] += get_gradient_ij(X,y,parameters,j,model) #???\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjNQ5Ulm_Itl",
        "outputId": "d932fed4-8cf0-40ab-a3d7-7ee0a0a33bf7"
      },
      "source": [
        "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
        "gradients1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37.04643911461055, 9.629822347366838, 42.93911094489016]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDqaW5he_Itl"
      },
      "source": [
        "## mini-batch\n",
        "인덱스로 미니 배치 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVEs5rF_Itl"
      },
      "source": [
        "def batch_idx(X_train, batch_size):\n",
        "    N = len(X_train)\n",
        "    nb = (N // batch_size)+1 #number of batch\n",
        "    idx = np.array([i for i in range(N)])\n",
        "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
        "\n",
        "    return idx_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkMKFMwi_Itl"
      },
      "source": [
        "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
        "### 설명: 총 데이터 갯수와 총 batch의 갯수로 나눠 크기를 정하고, for문을 이용해 idx_list에 그 갯수 단위로 나누어 배치 안에서 들어갈 원래 데이터의 인덱스를 할당"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duy5T41K_Itm"
      },
      "source": [
        "## Update Parameters\n",
        "기울기를 갱신하는 코드를 작성해주세요  \n",
        "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTccq-QG_Itm"
      },
      "source": [
        "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
        "\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "        gradients[i] *= learning_rate/n\n",
        "    \n",
        "    parameters -= gradients\n",
        "    return parameters"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7_D2Og5_Itm",
        "outputId": "84abc34b-7a4a-484d-ebd3-5dc46028eb57"
      },
      "source": [
        "step(parameters, gradients1, 0.01, len(X_train))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.14054443, 0.40187686, 0.60513811])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMXjTy0d_Itm"
      },
      "source": [
        "## Gradient Descent\n",
        "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
        "\n",
        "- learning_rate: 학습률  \n",
        "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
        "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
        "- epoch:  주어진 반복 횟수만큼 반복함\n",
        "- num_epoch:반복 횟수\n",
        "<br>\n",
        "\n",
        "BGD: 학습 한 번에 모든 데이터셋에 대해 기울기를 구한다<br>\n",
        "SGD: 학습 한 번에 임의의 데이터에 대해서만 기울기를 구한다<br>\n",
        "MGD:학습 한 번에 데이터셋의 일부에 대해서만 기울기를 구한다<br>\n",
        "<br>\n",
        "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
        "batch_size=1 -> SGD\n",
        "batch_size=k -> MGD\n",
        "batch_size=whole -> BGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp88YWjH_Itm"
      },
      "source": [
        "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
        "    stopper = False\n",
        "    \n",
        "    N = len(X_train.iloc[0])\n",
        "    parameters = np.random.rand(N)\n",
        "\n",
        "    loss_function_set = minus_log_cross_entropy_i if model == 'logistic' else mse_i  \n",
        "   \n",
        "\n",
        "    loss = 999\n",
        "    batch_idx_list = batch_idx(X_train, batch_size)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        if stopper:\n",
        "            break\n",
        "        for idx in batch_idx_list:\n",
        "            X_batch = X_train.iloc[idx,:]\n",
        "            y_batch = y_train.iloc[idx]\n",
        "\n",
        "            gradients = batch_gradient(X_batch,y_batch,parameters,model)\n",
        "            parameters = step(parameters,gradients,learning_rate,batch_size)\n",
        "            new_loss = batch_loss(X_batch,y_batch,parameters,loss_function_set,batch_size)\n",
        "            \n",
        "          #중단 조건\n",
        "            if (abs(new_loss - loss) < tolerance):\n",
        "                stopper = True\n",
        "                break\n",
        "            loss = new_loss\n",
        "        \n",
        "        #100epoch마다 학습 상태 출력\n",
        "        if epoch%10 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
        "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrneE6a5_Itn"
      },
      "source": [
        "## Implement\n",
        "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfFC0_cR_Itn"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnTe1RI6_Itn",
        "outputId": "0c84c1b5-94c3-41e3-8611-0f9b38b3f5ae"
      },
      "source": [
        "new_param_bgd = gradient_descent(X_train, y_train,tolerance=0.0001,learning_rate=0.03,batch_size=X_train.shape[0])\n",
        "new_param_bgd"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: 1.132858431163191  params: [0.86002647 0.91768245 0.68375025]  gradients: [0.010790433921284882, 0.0037047968242751358, 0.009599732878739296]\n",
            "epoch: 10  loss: 1.0613114043248266  params: [0.75393806 0.88168227 0.58916968]  gradients: [0.010455871765653718, 0.003509825745995656, 0.009336938537963706]\n",
            "epoch: 20  loss: 0.9944558187840473  params: [0.65135672 0.84779765 0.49741054]  gradients: [0.010091961944771749, 0.0032845654541531813, 0.009038786204360333]\n",
            "epoch: 30  loss: 0.9325925761440709  params: [0.55257313 0.81633383 0.40883011]  gradients: [0.00969941991672509, 0.003028803007296424, 0.008704843132285171]\n",
            "epoch: 40  loss: 0.8759483466920993  params: [0.45786271 0.7875895  0.32378012]  gradients: [0.0092804716214933, 0.002743967952444619, 0.00833655248822151]\n",
            "epoch: 50  loss: 0.8246480127867292  params: [0.36746946 0.76183828 0.24258541]  gradients: [0.008839022107003254, 0.0024334869522464114, 0.007937670913973616]\n",
            "epoch: 60  loss: 0.7786929270455819  params: [0.28158916 0.73930787 0.16551958]  gradients: [0.008380614084322049, 0.002102896955436155, 0.007514441249410496]\n",
            "epoch: 70  loss: 0.7379506453085746  params: [0.20035452 0.72016002 0.09278108]  gradients: [0.007912115459796665, 0.0017595911277920695, 0.0070753417434011165]\n",
            "epoch: 80  loss: 0.7021601498783115  params: [0.12382487 0.70447525 0.02447456]  gradients: [0.007441144853270073, 0.001412164397887428, 0.006630353683910755]\n",
            "epoch: 90  loss: 0.6709529014573682  params: [ 0.05198263  0.69224586 -0.03939841]  gradients: [0.006975336648340284, 0.001069470670805594, 0.0061898630932682485]\n",
            "epoch: 100  loss: 0.643885673069045  params: [-0.01526273  0.68337889 -0.09893669]  gradients: [0.006521614657075029, 0.0007396267663126135, 0.005763470893443774]\n",
            "epoch: 110  loss: 0.6204780926121901  params: [-0.07806444  0.67770811 -0.15432619]  gradients: [0.006085642040975855, 0.0004292201699522122, 0.005359026598241408]\n",
            "epoch: 120  loss: 0.6002475400706161  params: [-0.13662399  0.675012   -0.20581897]  gradients: [0.0056715432705170855, 0.00014288417533579316, 0.004982095266724408]\n",
            "epoch: 130  loss: 0.5827362537172711  params: [-0.1911755   0.67503374 -0.25370969]  gradients: [0.005281895865715436, -0.0001167420650182556, 0.004635889425519272]\n",
            "epoch: 140  loss: 0.5675287000267315  params: [-0.2419712   0.67750005 -0.29831342]  gradients: [0.0049179175733836, -0.0003487661812012749, 0.004321552621340498]\n",
            "epoch: 150  loss: 0.5542599332772719  params: [-0.28926933  0.68213649 -0.33994745]  gradients: [0.004579751491213918, -0.0005536480334058, 0.004038625149637451]\n",
            "epoch: 160  loss: 0.5426171034812035  params: [-0.33332507  0.68867869 -0.37891798]  gradients: [0.0042667673976433595, -0.0007327792279853177, 0.003785543649612654]\n",
            "epoch: 170  loss: 0.5323365665729127  params: [-0.3743843   0.69687955 -0.41551161]  gradients: [0.003977829014297559, -0.0008881124039843425, 0.0035600817251447057]\n",
            "epoch: 180  loss: 0.5231986744928147  params: [-0.4126798   0.70651321 -0.44999069]  gradients: [0.0037115060611105956, -0.0010218722507976976, 0.0033596925018893613]\n",
            "epoch: 190  loss: 0.5150217171068513  params: [-0.44842913  0.71737653 -0.48259148]  gradients: [0.0034662293259614357, -0.0011363507190221805, 0.0031817502409753062]\n",
            "epoch: 200  loss: 0.5076559189053043  params: [-0.48183396  0.72928905 -0.51352434]  gradients: [0.0032403966702042506, -0.0012337740518481852, 0.003023706195506697]\n",
            "epoch: 210  loss: 0.5009779647334035  params: [-0.51308005  0.74209181 -0.54297498]  gradients: [0.0030324409633154465, -0.001316224637147903, 0.0028831793889351038]\n",
            "epoch: 220  loss: 0.49488624819580623  params: [-0.5423379   0.75564568 -0.57110647]  gradients: [0.00284087046807999, -0.0013856015371563227, 0.0027580018034625113]\n",
            "epoch: 230  loss: 0.48929687286529316  params: [-0.56976359  0.76982946 -0.59806144]  gradients: [0.002664290309179245, -0.0014436065884410381, 0.002646233679209717]\n",
            "epoch: 240  loss: 0.4841403525391785  params: [-0.59549986  0.78453786 -0.62396438]  gradients: [0.0025014115190522216, -0.001491746357552388, 0.002546160466170212]\n",
            "epoch: 250  loss: 0.47935892162793836  params: [-0.61967713  0.79967965 -0.64892379]  gradients: [0.0023510522771819776, -0.001531343189628536, 0.002456279394290033]\n",
            "epoch: 260  loss: 0.4749043587709038  params: [-0.64241459  0.81517589 -0.67303426]  gradients: [0.0022121344929985254, -0.001563550869102313, 0.002375280889931434]\n",
            "epoch: 270  loss: 0.4707362323841197  params: [-0.66382117  0.83095831 -0.69637822]  gradients: [0.002083677813134564, -0.0015893720551922426, 0.0023020281145387633]\n",
            "epoch: 280  loss: 0.4668204881658342  params: [-0.68399653  0.84696792 -0.71902759]  gradients: [0.0019647923872476106, -0.0016096757825533915, 0.0022355365745804472]\n",
            "epoch: 290  loss: 0.46312831136484334  params: [-0.70303187  0.86315379 -0.7410452 ]  gradients: [0.0018546712215789503, -0.0016252140626780785, 0.0021749548838158415]\n",
            "epoch: 300  loss: 0.45963520877337166  params: [-0.72101075  0.87947191 -0.76248601]  gradients: [0.0017525826159392284, -0.0016366370979859907, 0.002119547210004033]\n",
            "epoch: 310  loss: 0.4563202660712986  params: [-0.73800979  0.89588428 -0.7833982 ]  gradients: [0.001657862964186682, -0.0016445069147627786, 0.002068677603199063]\n",
            "epoch: 320  loss: 0.4531655450917849  params: [-0.75409933  0.91235811 -0.80382404]  gradients: [0.0015699100617026136, -0.0016493093954218013, 0.0020217962072180346]\n",
            "epoch: 330  loss: 0.450155592876029  params: [-0.76934398  0.92886509 -0.82380075]  gradients: [0.0014881769788937865, -0.0016514647878754696, 0.0019784272488879862]\n",
            "epoch: 340  loss: 0.4472770402396686  params: [-0.78380317  0.94538078 -0.84336115]  gradients: [0.0014121665090744385, -0.0016513368188684277, 0.0019381586475500521]\n",
            "epoch: 350  loss: 0.44451827222152107  params: [-0.79753159  0.96188412 -0.86253421]  gradients: [0.0013414261700230251, -0.0016492405578093128, 0.001900633068131401]\n",
            "epoch: 360  loss: 0.44186915645275526  params: [-0.81057965  0.97835697 -0.88134564]  gradients: [0.0012755437230684164, -0.0016454491800873465, 0.001865540240939198]\n",
            "epoch: 370  loss: 0.43932081836876913  params: [-0.82299383  0.99478371 -0.89981825]  gradients: [0.001214143166537495, -0.0016401997717543407, 0.0018326103814671497]\n",
            "epoch: 380  loss: 0.4368654544516617  params: [-0.83481704  1.0111509  -0.91797234]  gradients: [0.0011568811584455358, -0.001633698305550225, 0.0018016085585988136]\n",
            "epoch: 390  loss: 0.4344961764715292  params: [-0.84608892  1.02744704 -0.93582605]  gradients: [0.0011034438243028963, -0.001626123904425379, 0.0017723298764669038]\n",
            "epoch: 400  loss: 0.43220688109586214  params: [-0.8568461   1.04366228 -0.9533956 ]  gradients: [0.0010535439085070347, -0.0016176324946419413, 0.0017445953520677136]\n",
            "epoch: 410  loss: 0.42999214034136  params: [-0.8671225   1.05978819 -0.97069556]  gradients: [0.0010069182311583957, -0.001608359937142998, 0.0017182483865598782]\n",
            "epoch: 420  loss: 0.4278471092165528  params: [-0.87694951  1.07581765 -0.98773904]  gradients: [0.0009633254157804633, -0.0015984247136251685, 0.001693151742533524]\n",
            "epoch: 430  loss: 0.4257674475972238  params: [-0.88635623  1.0917446  -1.00453785]  gradients: [0.0009225438570437091, -0.0015879302328198905, 0.001669184952259162]\n",
            "epoch: 440  loss: 0.4237492539290095  params: [-0.89536964  1.10756396 -1.02110271]  gradients: [0.0008843699010272038, -0.0015769668129035636, 0.0016462420930267825]\n",
            "epoch: 450  loss: 0.4217890087930721  params: [-0.90401477  1.12327147 -1.03744333]  gradients: [0.0008486162137180077, -0.0015656133876499783, 0.001624229875267417]\n",
            "epoch: 460  loss: 0.4198835267250627  params: [-0.91231487  1.13886364 -1.05356856]  gradients: [0.0008151103163145375, -0.0015539389767992048, 0.001603065997357776]\n",
            "epoch: 470  loss: 0.41802991496298314  params: [-0.92029152  1.15433758 -1.06948647]  gradients: [0.0007836932684623011, -0.001542003955017088, 0.0015826777280032933]\n",
            "epoch: 480  loss: 0.41622553803038004  params: [-0.92796479  1.16969098 -1.08520446]  gradients: [0.0007542184828219125, -0.0015298611486291175, 0.001563000683033772]\n",
            "epoch: 490  loss: 0.41446798724869005  params: [-0.93535337  1.18492204 -1.10072933]  gradients: [0.0007265506563713965, -0.0015175567849081604, 0.0015439777684758621]\n",
            "epoch: 500  loss: 0.4127550544252641  params: [-0.94247461  1.20002935 -1.11606732]  gradients: [0.000700564805602873, -0.0015051313149655188, 0.0015255582660205754]\n",
            "epoch: 510  loss: 0.4110847090885265  params: [-0.94934471  1.21501191 -1.13122422]  gradients: [0.0006761453943136936, -0.001492620128138955, 0.001507697040598555]\n",
            "epoch: 520  loss: 0.40945507874426906  params: [-0.95597872  1.22986903 -1.14620539]  gradients: [0.0006531855440389654, -0.0014800541731028684, 0.0014903538528122354]\n",
            "epoch: 530  loss: 0.4078644317115882  params: [-0.9623907   1.24460032 -1.16101581]  gradients: [0.0006315863183497518, -0.0014674604986695838, 0.0014734927615392998]\n",
            "epoch: 540  loss: 0.40631116216682045  params: [-0.96859373  1.25920563 -1.17566012]  gradients: [0.0006112560732699676, -0.0014548627253422022, 0.0014570816041901869]\n",
            "epoch: 550  loss: 0.40479377708176667  params: [-0.97460004  1.27368504 -1.19014265]  gradients: [0.0005921098669644532, -0.001442281457064635, 0.0014410915439361737]\n",
            "epoch: 560  loss: 0.4033108847907069  params: [-0.98042104  1.28803881 -1.20446748]  gradients: [0.0005740689226372183, -0.001429734641246449, 0.0014254966747772662]\n",
            "epoch: 570  loss: 0.4018611849609456  params: [-0.98606736  1.30226738 -1.21863842]  gradients: [0.0005570601392675412, -0.0014172378839804967, 0.0014102736766348173]\n",
            "epoch: 580  loss: 0.4004434597752934  params: [-0.99154894  1.31637132 -1.23265908]  gradients: [0.0005410156454149895, -0.001404804726387013, 0.0013954015137702285]\n",
            "epoch: 590  loss: 0.3990565661631655  params: [-0.99687509  1.33035133 -1.24653286]  gradients: [0.0005258723918538267, -0.001392446887181406, 0.0013808611707795341]\n",
            "epoch: 600  loss: 0.39769942894076576  params: [-1.00205449  1.34420823 -1.26026297]  gradients: [0.0005115717792622459, -0.0013801744758510442, 0.0013666354212206367]\n",
            "epoch: 610  loss: 0.3963710347409038  params: [-1.00709526  1.35794291 -1.27385249]  gradients: [0.0004980593176009159, -0.001367996180219981, 0.0013527086246172323]\n",
            "epoch: 620  loss: 0.395070426629952  params: [-1.012005    1.37155636 -1.28730432]  gradients: [0.0004852843141755638, -0.0013559194316626834, 0.001339066548170116]\n",
            "epoch: 630  loss: 0.393796699323874  params: [-1.01679083  1.38504964 -1.30062123]  gradients: [0.0004731995876960747, -0.0013439505507856732, 0.001325696210007475]\n",
            "epoch: 640  loss: 0.3925489949274572  params: [-1.0214594   1.39842384 -1.31380587]  gradients: [0.00046176120592529964, -0.0013320948760171436, 0.0013125857412346913]\n",
            "epoch: 650  loss: 0.39132649913131773  params: [-1.02601695  1.41168013 -1.32686079]  gradients: [0.0004509282447590596, -0.0013203568772201192, 0.0012997242644115605]\n",
            "epoch: 660  loss: 0.39012843781012324  params: [-1.03046931  1.42481971 -1.33978841]  gradients: [0.000440662566798802, -0.0013087402561659442, 0.001287101786400238]\n",
            "epoch: 670  loss: 0.388954073973075  params: [-1.03482198  1.4378438  -1.35259109]  gradients: [0.00043092861767356276, -0.0012972480354653125, 0.001274709103798244]\n",
            "epoch: 680  loss: 0.38780270502420994  params: [-1.03908007  1.45075366 -1.36527106]  gradients: [0.00042169323854131046, -0.0012858826373475733, 0.0012625377194042625]\n",
            "epoch: 690  loss: 0.3866736602956495  params: [-1.04324841  1.46355058 -1.37783049]  gradients: [0.0004129254933540029, -0.001274645953501067, 0.0012505797683655686]\n",
            "epoch: 700  loss: 0.38556629882173776  params: [-1.0473315   1.47623585 -1.39027149]  gradients: [0.00040459650960826863, -0.001263539407033499, 0.00123882795282956]\n",
            "epoch: 710  loss: 0.38448000732613513  params: [-1.0513336   1.48881077 -1.40259607]  gradients: [0.00039667933142625394, -0.0012525640074782139, 0.0012272754840720631]\n",
            "epoch: 720  loss: 0.3834141983974992  params: [-1.05525866  1.50127666 -1.41480619]  gradients: [0.00038914878392090185, -0.0012417203996571101, 0.0012159160312048937]\n",
            "epoch: 730  loss: 0.38236830883246226  params: [-1.05911044  1.51363484 -1.42690375]  gradients: [0.00038198134789793847, -0.0012310089071107982, 0.0012047436756779876]\n",
            "epoch: 740  loss: 0.38134179812728175  params: [-1.06289243  1.52588663 -1.43889059]  gradients: [0.0003751550440348827, -0.0012204295707197938, 0.001193752870889026]\n",
            "epoch: 750  loss: 0.38033414710183466  params: [-1.06660795  1.53803336 -1.45076849]  gradients: [0.0003686493257562171, -0.0012099821830648724, 0.001182938406298605]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.06660795,  1.53803336, -1.45076849])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YkqKk65_Itn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6345c666-8e5a-42f4-f999-d4203b247fef"
      },
      "source": [
        "new_param_sgd = gradient_descent(X_train, y_train,model='logistic',batch_size=1,learning_rate=0.01,num_epoch=100)\n",
        "new_param_sgd"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: 0.9686117682923773  params: [ 0.24639033  0.74900115 -0.23137331]  gradients: [0.006230148977556198, 0.0033904530325247627, 0.004383475140578348]\n",
            "epoch: 10  loss: 0.27108616692243365  params: [-0.99498152  1.38337902 -1.31407359]  gradients: [0.002382226975165722, 0.0012964101984092658, 0.0016761128445679935]\n",
            "epoch: 20  loss: 0.21569989077594273  params: [-1.1762817   1.92980241 -1.84481969]  gradients: [0.0019456842725942606, 0.0010588432421307125, 0.0013689654406429117]\n",
            "epoch: 30  loss: 0.1868202646053056  params: [-1.28428113  2.31203229 -2.21246996]  gradients: [0.0017084020543336592, 0.0009297140320004236, 0.0012020158686834588]\n",
            "epoch: 40  loss: 0.1674115968217083  params: [-1.36727036  2.59905683 -2.48662368]  gradients: [0.0015450981038717029, 0.0008408438659640097, 0.0010871167210407624]\n",
            "epoch: 50  loss: 0.15336939701673083  params: [-1.43406452  2.82485523 -2.7010954 ]  gradients: [0.0014249881466347994, 0.0007754799123543458, 0.0010026084671321347]\n",
            "epoch: 60  loss: 0.1427526280754813  params: [-1.48905014  3.00808348 -2.87435352]  gradients: [0.0013330712901409884, 0.0007254586711348737, 0.000937936617888688]\n",
            "epoch: 70  loss: 0.13446384089395727  params: [-1.53507405  3.16005275 -3.01752759]  gradients: [0.0012606407601881838, 0.000686041907457067, 0.0008869751675909822]\n",
            "epoch: 80  loss: 0.1278310063737422  params: [-1.57410654  3.28814539 -3.13784004]  gradients: [0.0012022550815573522, 0.00065426836530213, 0.0008458955446532272]\n",
            "epoch: 90  loss: 0.12241912030991046  params: [-1.60756091  3.39745297 -3.24024593]  gradients: [0.0011543349328566365, 0.0006281901745451076, 0.0008121793716822238]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.63376905,  3.48281783, -3.32005462])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFFnGadF_Ito",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e24ae5-ad82-4936-bf0e-f66b90a6c2fe"
      },
      "source": [
        "new_param_mgd = gradient_descent(X_train, y_train, tolerance=0.00001,batch_size=20, learning_rate=0.005)\n",
        "new_param_mgd"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: 0.582885902011296  params: [0.37002599 0.29885476 0.78293868]  gradients: [0.0005949617857954079, 0.0009732339821322349, 0.0014552665572489386]\n",
            "epoch: 10  loss: 0.5364383670777683  params: [0.26408869 0.27862956 0.67473475]  gradients: [0.0005377613183814862, 0.0008978126532427571, 0.0013790429692912293]\n",
            "epoch: 20  loss: 0.4964988802710244  params: [0.16516524 0.26349141 0.57277854]  gradients: [0.0004786448789580005, 0.0008183661819979328, 0.0012983608025613613]\n",
            "epoch: 30  loss: 0.46274221001017013  params: [0.07337329 0.25355968 0.47738743]  gradients: [0.0004193565520994279, 0.0007375531553612109, 0.0012159692123910723]\n",
            "epoch: 40  loss: 0.4346234929697788  params: [-0.01133067  0.24877277  0.38866783]  gradients: [0.0003616092298350988, 0.0006580986659687482, 0.0011346742458274576]\n",
            "epoch: 50  loss: 0.4114522408071517  params: [-0.08914325  0.24889576  0.30651008]  gradients: [0.000306839645622835, 0.0005823669585631307, 0.001056907828544952]\n",
            "epoch: 60  loss: 0.3924785247174765  params: [-0.16038797  0.25355242  0.23061551]  gradients: [0.0002560602534960961, 0.0005120800713650521, 0.0009844458519245511]\n",
            "epoch: 70  loss: 0.3769686783317233  params: [-0.22547823  0.26227196  0.16054599]  gradients: [0.00020983132734956895, 0.0004482371239954506, 0.0009183281169930886]\n",
            "epoch: 80  loss: 0.3642571753293563  params: [-0.28487878  0.27453821  0.09578104]  gradients: [0.00016832451562293993, 0.0003911960648266418, 0.0008589413041188294]\n",
            "epoch: 90  loss: 0.3537726963403802  params: [-0.33907222  0.28983147  0.03576977]  gradients: [0.00013142844644384717, 0.0003408373377505582, 0.0008061843904568214]\n",
            "epoch: 100  loss: 0.34504385133693977  params: [-0.38853418  0.30765855 -0.02002959]  gradients: [9.885538990188896e-05, 0.00029673808858616727, 0.0007596450298737235]\n",
            "epoch: 110  loss: 0.3376923030703316  params: [-0.43371704  0.32757057 -0.07212418]  gradients: [7.022729328578298e-05, 0.00025831665442560504, 0.0007187463390827528]\n",
            "epoch: 120  loss: 0.33141995408027025  params: [-0.47504093  0.34917108 -0.12097285]  gradients: [4.513568137225898e-05, 0.0002249350603374236, 0.0006828514906062677]\n",
            "epoch: 130  loss: 0.32599463532988754  params: [-0.51288991  0.37211735 -0.16698126]  gradients: [2.317865500674818e-05, 0.00019596324002449055, 0.0006513296240433041]\n",
            "epoch: 140  loss: 0.32123670414996325  params: [-0.54761148  0.39611754 -0.21050246]  gradients: [3.981048803420956e-06, 0.00017081445978095613, 0.0006235924943604454]\n",
            "epoch: 150  loss: 0.31700756430136456  params: [-0.57951805  0.42092584 -0.25184058]  gradients: [-1.27964765445603e-05, 0.00014896151538011508, 0.0005991114672922127]\n",
            "epoch: 160  loss: 0.31320032389430585  params: [-0.60888947  0.44633683 -0.29125597]  gradients: [-2.7454995484090554e-05, 0.00012994122261087533, 0.0005774224695511297]\n",
            "epoch: 170  loss: 0.3097324239926408  params: [-0.6359759   0.47217976 -0.3289707 ]  gradients: [-4.025854080981159e-05, 0.00011335239724667241, 0.0005581241842867159]\n",
            "epoch: 180  loss: 0.30653992932911567  params: [-0.66100082  0.49831332 -0.36517395]  gradients: [-5.143711567954415e-05, 9.885062424340479e-05, 0.0005408728739976903]\n",
            "epoch: 190  loss: 0.3035731537835706  params: [-0.68416391  0.52462094 -0.40002686]  gradients: [-6.119039436932179e-05, 8.614177675088703e-05, 0.0005253758550281312]\n",
            "epoch: 200  loss: 0.3007933274514242  params: [-0.70564364  0.55100675 -0.43366689]  gradients: [-6.969150606889939e-05, 7.497537547179819e-05, 0.000511384759334564]\n",
            "epoch: 210  loss: 0.29817006292805837  params: [-0.72559968  0.57739211 -0.46621151]  gradients: [-7.709059519446738e-05, 6.513834552521994e-05, 0.000498689170737165]\n",
            "epoch: 220  loss: 0.29567942902072325  params: [-0.74417505  0.60371269 -0.49776139]  gradients: [-8.351802453338605e-05, 5.6449416339882045e-05, 0.00048711089997456727]\n",
            "epoch: 230  loss: 0.293302483929107  params: [-0.76149796  0.6299161  -0.52840309]  gradients: [-8.908718267927296e-05, 4.875423676320696e-05, 0.00047649898166334214]\n",
            "epoch: 240  loss: 0.29102415541863647  params: [-0.77768353  0.6559598  -0.55821128]  gradients: [-9.389690637850397e-05, 4.192118707828922e-05, 0.00046672538052564824]\n",
            "epoch: 250  loss: 0.28883238317770327  params: [-0.7928352   0.68180948 -0.58725062]  gradients: [-9.803355127435773e-05, 3.583782703946069e-05, 0.00045768134833429314]\n",
            "epoch: 260  loss: 0.28671745965860007  params: [-0.80704606  0.70743764 -0.61557735]  gradients: [-0.0001015727529089625, 3.040790314426306e-05, 0.00044927435511770905]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.81537437,  0.7221151 , -0.63085214])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stsoveat_Ito"
      },
      "source": [
        "### Predict Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D17JMoUA_Ito"
      },
      "source": [
        "y_predict = []\n",
        "for i in range(len(y_test)):\n",
        "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
        "    if p> 0.5 :\n",
        "        y_predict.append(1)\n",
        "    else :\n",
        "        y_predict.append(0)\n",
        "y_predict_random = []\n",
        "for i in range(len(y_test)):\n",
        "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
        "    if p> 0.5 :\n",
        "        y_predict_random.append(1)\n",
        "    else :\n",
        "        y_predict_random.append(0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_KyFbDV_Ito"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcngOThr_Itp"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fj37Exj_Itp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04df3576-a534-4ddc-8c17-128999f9a8b1"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
        "confusion_matrix(y_test, y_predict)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[38,  2],\n",
              "       [ 7,  3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h62XYwH_Itq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6f0b3e-d77b-430a-f519-883e31290a59"
      },
      "source": [
        "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
        "print(\"accuracy:\",accuracy)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGWeF92e_Itq"
      },
      "source": [
        "## Linear regression\n",
        "### $y = 0.5 + 2.7x$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bk6xJD8_Itq"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiEWLPB8_Itq"
      },
      "source": [
        "raw_X = np.random.rand(150)\n",
        "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J9TqvKZ_Itr"
      },
      "source": [
        "tmp = np.array([1 for _ in range(150)])\n",
        "X = np.vstack((tmp, raw_X)).T\n",
        "X = pd.DataFrame(X)\n",
        "y = pd.Series(y)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HUEi5rF_Itr"
      },
      "source": [
        "### Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_kfTiAq_Itr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276dd543-bf0e-4a46-9b93-4606fbae9283"
      },
      "source": [
        "#정규방정식\n",
        "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
        "theta"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.517723  , 2.72396608])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V665yRIp_Itr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febd22bc-05c3-483f-9808-40a441fb13c3"
      },
      "source": [
        "#경사하강법\n",
        "new_param = gradient_descent(X, y,learning_rate=0.05,num_epoch=50)\n",
        "new_param"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: -0.3522110746648978  params: [0.60925953 1.21386696]  gradients: [-0.023397863852266578, -0.011845987472647299]\n",
            "epoch: 10  loss: -2.4579023080004787  params: [4.46844001 4.03198742]  gradients: [-0.01822764306142353, -0.010403057482114585]\n",
            "epoch: 20  loss: -4.335971648015096  params: [8.07515543 6.76351563]  gradients: [-0.018149731536910442, -0.010386562931370942]\n",
            "epoch: 30  loss: -6.211139521738419  params: [11.67842191  9.49435577]  gradients: [-0.018148492560256434, -0.01038633114859561]\n",
            "epoch: 40  loss: -8.086260086188476  params: [15.28162862 12.22518755]  gradients: [-0.0181484719729048, -0.010386327520605736]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18.52451361, 14.68293605])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ5H81IR_Itr"
      },
      "source": [
        "y_hat_NE = theta.dot(X.T)\n",
        "y_hat_GD = new_param.dot(X.T)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbxJeEk_Its"
      },
      "source": [
        "### Visualization\n",
        "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
        "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nECMOOOJ_Its",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "534fb707-b681-4d10-d26c-11fb8c2095b1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
        "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
        "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWSH3yDC_Its"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}