{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "[2]Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "782pDvqa_Itg"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JangAyeon/ToBigs/blob/master/%5B2%5DOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90grJPRt_ItX"
      },
      "source": [
        "# Tobig's 15기 2주차 Optimization 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEyXE6gV_Itc"
      },
      "source": [
        "# Gradient Descent 구현하기\n",
        "\n",
        "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
        "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422BL0TT_Itd"
      },
      "source": [
        "## 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bPHAVuA_Itd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyyhxhfx_Ite",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "67cac6f1-2d43-4d8f-ccc2-f3e5ba223e8e"
      },
      "source": [
        "data = pd.read_csv('assignment_2.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7</td>\n",
              "      <td>48000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>48000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.2</td>\n",
              "      <td>63000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>76000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label  bias  experience  salary\n",
              "0      1     1         0.7   48000\n",
              "1      0     1         1.9   48000\n",
              "2      1     1         2.5   60000\n",
              "3      0     1         4.2   63000\n",
              "4      0     1         6.0   76000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ayYoXQ4_Ite"
      },
      "source": [
        "## Train Test 데이터 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI4GZgfN_Ite"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6QpRYCC_Ite"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIFhpP7I_Itf",
        "outputId": "2032d17e-877f-4910-e584-c44dae6c6def"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 3), (50, 3), (150,), (50,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "YDpeZsm2NWuQ",
        "outputId": "6f309bc3-76b0-41f0-a440-65217c7562a7"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>1</td>\n",
              "      <td>5.3</td>\n",
              "      <td>48000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>1</td>\n",
              "      <td>8.1</td>\n",
              "      <td>66000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>1</td>\n",
              "      <td>3.9</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>45000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>66000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>64000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>1</td>\n",
              "      <td>4.8</td>\n",
              "      <td>73000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>86000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>7.6</td>\n",
              "      <td>78000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>1</td>\n",
              "      <td>6.3</td>\n",
              "      <td>76000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     bias  experience  salary\n",
              "71      1         5.3   48000\n",
              "124     1         8.1   66000\n",
              "184     1         3.9   60000\n",
              "97      1         0.2   45000\n",
              "149     1         1.1   66000\n",
              "..    ...         ...     ...\n",
              "67      1         6.7   64000\n",
              "192     1         4.8   73000\n",
              "117     1         7.0   86000\n",
              "47      1         7.6   78000\n",
              "172     1         6.3   76000\n",
              "\n",
              "[150 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HoONpPh_Itf"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "Qqlmm5s6_Itf",
        "outputId": "8414b583-20af-4717-a31b-2ed0c6e99ce9"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "bias_train = X_train[\"bias\"]\n",
        "bias_train = bias_train.reset_index()[\"bias\"]\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "X_train[\"bias\"] = bias_train\n",
        "X_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.187893</td>\n",
              "      <td>-1.143335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.185555</td>\n",
              "      <td>0.043974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.310938</td>\n",
              "      <td>-0.351795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.629277</td>\n",
              "      <td>-1.341220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.308600</td>\n",
              "      <td>0.043974</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bias  experience    salary\n",
              "0     1    0.187893 -1.143335\n",
              "1     1    1.185555  0.043974\n",
              "2     1   -0.310938 -0.351795\n",
              "3     1   -1.629277 -1.341220\n",
              "4     1   -1.308600  0.043974"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5idcw4Iw_Itf"
      },
      "source": [
        "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
        "똑같이 X_test에다 fit하면 안돼요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "nFA2nI4g_Itg",
        "outputId": "6b68859d-2708-4fc1-a163-22a699c8945d"
      },
      "source": [
        "bias_test = X_test[\"bias\"]\n",
        "bias_test = bias_test.reset_index()[\"bias\"]\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
        "X_test[\"bias\"] = bias_test\n",
        "X_test.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>experience</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.344231</td>\n",
              "      <td>-0.615642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.508570</td>\n",
              "      <td>0.307821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.310938</td>\n",
              "      <td>0.571667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.363709</td>\n",
              "      <td>1.956862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.987923</td>\n",
              "      <td>-0.747565</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bias  experience    salary\n",
              "0     1   -1.344231 -0.615642\n",
              "1     1    0.508570  0.307821\n",
              "2     1   -0.310938  0.571667\n",
              "3     1    1.363709  1.956862\n",
              "4     1   -0.987923 -0.747565"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5N0yc9K_Itg"
      },
      "source": [
        "# parameter 개수\n",
        "N = len(X_train.loc[0])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR6JjcVr_Itg",
        "outputId": "18e8597b-70ec-40f0-c4d5-a1ba0e8318a7"
      },
      "source": [
        "# 초기 parameter들을 임의로 설정해줍니다.\n",
        "parameters = np.array([random.random() for i in range(N)])\n",
        "random_parameters = parameters.copy()\n",
        "parameters"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6139545 , 0.57595081, 0.5653124 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782pDvqa_Itg"
      },
      "source": [
        "### * LaTeX   \n",
        "\n",
        "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
        "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
        "http://triki.net/apps/3466  \n",
        "https://jjycjnmath.tistory.com/117"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icE0hEAZ_Ith"
      },
      "source": [
        "## Dot product\n",
        "## $z = X_i \\theta$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KodweEt_Ith"
      },
      "source": [
        "def dot_product(X, parameters):\n",
        "    z = 0\n",
        "    for i in range(len(parameters)):\n",
        "        z +=X[i]*parameters.T[i]\n",
        "    return z"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5EDfifh_Ith"
      },
      "source": [
        "## Logistic Function\n",
        "\n",
        "## $p =\\frac{1}{1+e^{-Z}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNxzHG-S_Ith"
      },
      "source": [
        "def logistic(X, parameters):\n",
        "    z = dot_product(X,parameters)\n",
        "    p =np.exp(z)/(1 + np.exp(z))   \n",
        "    return p"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlV8rkCb_Iti",
        "outputId": "16ffef3b-3646-401e-cd57-025063551532"
      },
      "source": [
        "logistic(X_train.iloc[1], parameters)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7894536435880117"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0yoG93_Iti"
      },
      "source": [
        "## Object function\n",
        "\n",
        "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
        "<br>\n",
        "선형 회귀의 목적함수\n",
        "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
        "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
        "  \n",
        "로지스틱 회귀의 목적함수를 작성해주세요  \n",
        "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
        "## $l(p) = -\\Sigma((y_ilog p(X_i))+(i-y_i)log(1-p(X_i)))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIWmO8w-_Iti"
      },
      "source": [
        "def minus_log_cross_entropy_i(X, y, parameters):\n",
        "    p = logistic(X,parameters)\n",
        "    loss =-np.sum((y*np.log(p)+(1-y)*np.log(1-p)))\n",
        "    return loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trKQWp0__Iti"
      },
      "source": [
        "def mse_i(X, y, parameters):\n",
        "    y_hat = dot_product(X,parameters)\n",
        "    loss = (np.sum((y-y_hat)**2))/2\n",
        "    return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE0mC-qz_Itj"
      },
      "source": [
        "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(X_set.shape[0]):\n",
        "        X = X_set.iloc[i,:]\n",
        "        y = y_set.iloc[i]\n",
        "        loss +=loss_function(X,y,parameters)\n",
        "    loss = loss/n #loss 평균값으로 계산\n",
        "    return loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoYuSq_x_Itj",
        "outputId": "8c53f292-5b55-48b4-8ebf-ecff1b807715"
      },
      "source": [
        "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2164738530369688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1xLcVl7_Itj"
      },
      "source": [
        "## Gradient\n",
        "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
        "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af12kyis_Itj"
      },
      "source": [
        "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=\\Sigma(y_i-\\theta^{T}X_i)X_{ik}$\n",
        "## ${\\partial\\over{\\partial \\theta_j}}l(p)=\\Sigma((y_i-p_i)X_{ij})$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q014mMmH_Itk"
      },
      "source": [
        "def get_gradient_ij(X, y, parameters, j, model):\n",
        "    if model == 'linear':\n",
        "        y_hat = np.dot(X,parameters.T)\n",
        "        gradient =(y-y_hat)*X[j]\n",
        "    else:\n",
        "        p =logistic(X,parameters)\n",
        "        gradient =(y-p) *X[j]\n",
        "    return -gradient"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P74GPqS9_Itk",
        "outputId": "168925dd-121f-4fcd-a69b-5f75e7cd9960"
      },
      "source": [
        "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.09038623335256453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyn-INfX_Itk"
      },
      "source": [
        "## Batch Gradient\n",
        "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8g7qzPM_Itk"
      },
      "source": [
        "def batch_gradient(X_set, y_set, parameters, model):\n",
        "    gradients = [0 for _ in range(len(parameters))]\n",
        "    \n",
        "    for i in range(X_set.shape[0]):\n",
        "        X = X_set.iloc[i,:]\n",
        "        y = y_set.iloc[i]\n",
        "        for j in range(len(parameters)):\n",
        "            gradients[j] += get_gradient_ij(X,y,parameters,j,model) #???\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjNQ5Ulm_Itl",
        "outputId": "d3206b70-2570-4538-8243-5e614525ebae"
      },
      "source": [
        "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
        "gradients1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50.656484601359104, 11.346499448171967, 42.71347392442528]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDqaW5he_Itl"
      },
      "source": [
        "## mini-batch\n",
        "인덱스로 미니 배치 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVEs5rF_Itl"
      },
      "source": [
        "def batch_idx(X_train, batch_size):\n",
        "    N = len(X_train)\n",
        "    nb = (N // batch_size)+1 #number of batch\n",
        "    idx = np.array([i for i in range(N)])\n",
        "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
        "\n",
        "    return idx_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkMKFMwi_Itl"
      },
      "source": [
        "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
        "### 설명: 총 데이터 갯수와 총 batch의 갯수로 나눠 크기를 정하고, for문을 이용해 idx_list에 그 갯수 단위로 나누어 배치 안에서 들어갈 원래 데이터의 인덱스를 할당"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duy5T41K_Itm"
      },
      "source": [
        "## Update Parameters\n",
        "기울기를 갱신하는 코드를 작성해주세요  \n",
        "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTccq-QG_Itm"
      },
      "source": [
        "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
        "\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "        gradients[i] *= learning_rate/n\n",
        "    \n",
        "    parameters -= gradients\n",
        "    return parameters"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7_D2Og5_Itm",
        "outputId": "3beae513-2cc3-4c2d-ea01-b744906a9dc3"
      },
      "source": [
        "step(parameters, gradients1, 0.01, len(X_train))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6105774 , 0.57519438, 0.56246483])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMXjTy0d_Itm"
      },
      "source": [
        "## Gradient Descent\n",
        "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
        "\n",
        "- learning_rate: 학습률  \n",
        "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
        "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
        "- epoch:  주어진 반복 횟수만큼 반복함\n",
        "- num_epoch:반복 횟수\n",
        "<br>\n",
        "\n",
        "BGD: 학습 한 번에 모든 데이터셋에 대해 기울기를 구한다<br>\n",
        "SGD: 학습 한 번에 임의의 데이터에 대해서만 기울기를 구한다<br>\n",
        "MGD:학습 한 번에 데이터셋의 일부에 대해서만 기울기를 구한다<br>\n",
        "<br>\n",
        "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
        "batch_size=1 -> SGD\n",
        "batch_size=k -> MGD\n",
        "batch_size=whole -> BGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp88YWjH_Itm"
      },
      "source": [
        "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
        "    stopper = False\n",
        "    \n",
        "    N = len(X_train.iloc[0])\n",
        "    parameters = np.random.rand(N)\n",
        "\n",
        "    loss_function_set = minus_log_cross_entropy_i if model == 'logistic' else mse_i  \n",
        "   \n",
        "\n",
        "    loss = 999\n",
        "    batch_idx_list = batch_idx(X_train, batch_size)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        if stopper:\n",
        "            break\n",
        "        for idx in batch_idx_list:\n",
        "            X_batch = X_train.iloc[idx,:]\n",
        "            y_batch = y_train.iloc[idx]\n",
        "\n",
        "            gradients = batch_gradient(X_batch,y_batch,parameters,model)\n",
        "            parameters = step(parameters,gradients,learning_rate,batch_size)\n",
        "            new_loss = batch_loss(X_batch,y_batch,parameters,loss_function_set,batch_size)\n",
        "            \n",
        "          #중단 조건\n",
        "            if (abs(new_loss - loss) < tolerance):\n",
        "                stopper = True\n",
        "                break\n",
        "            loss = new_loss\n",
        "        \n",
        "        #100epoch마다 학습 상태 출력\n",
        "        if epoch%10 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
        "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrneE6a5_Itn"
      },
      "source": [
        "## Implement\n",
        "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfFC0_cR_Itn"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnTe1RI6_Itn",
        "outputId": "941b1fd9-34ea-4fbd-c0b6-9a499914fb75"
      },
      "source": [
        "new_param_bgd = gradient_descent(X_train, y_train,tolerance=0.0001,learning_rate=0.03,batch_size=X_train.shape[0])\n",
        "new_param_bgd"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: 0.7889557561335965  params: [0.2055004  0.49389667 0.34639774]  gradients: [0.00794215646791486, 0.0014796841949678036, 0.007592035719542181]\n",
            "epoch: 10  loss: 0.7508038463969031  params: [0.12866117 0.48098233 0.27286938]  gradients: [0.007472553179156797, 0.0011372238437588826, 0.007156120410398112]\n",
            "epoch: 20  loss: 0.717306453166655  params: [0.05650702 0.47148318 0.20371781]  gradients: [0.0070060320268573355, 0.0007977309125394691, 0.006718496329453884]\n",
            "epoch: 30  loss: 0.6880603487890575  params: [-0.01103169  0.46532484  0.13890619]  gradients: [0.006549542639471629, 0.0004692084011453098, 0.006288860252644196]\n",
            "epoch: 40  loss: 0.6626123529832025  params: [-0.07408898  0.46235889  0.07830647]  gradients: [0.006109025126273928, 0.00015846634654519153, 0.005875732553582503]\n",
            "epoch: 50  loss: 0.6404934819085202  params: [-0.13285091  0.46237798  0.02171533]  gradients: [0.005689045752756571, -0.0001294035646268975, 0.005485744953260573]\n",
            "epoch: 60  loss: 0.6212478264630512  params: [-0.187541    0.46513427 -0.03112487]  gradients: [0.005292683945660926, -0.0003912067998360286, 0.0051233398699865465]\n",
            "epoch: 70  loss: 0.6044530741791825  params: [-0.2384058   0.47035793 -0.08050443]  gradients: [0.004921631404232691, -0.0006255214305052514, 0.004790849177846199]\n",
            "epoch: 80  loss: 0.5897322495443695  params: [-0.28570217  0.4777732  -0.12672682]  gradients: [0.004576423285570102, -0.0008323807240779384, 0.004488827579070093]\n",
            "epoch: 90  loss: 0.5767580767261076  params: [-0.32968731  0.48711085 -0.17009336]  gradients: [0.004256719415437624, -0.0010128817946457897, 0.004216496361533564]\n",
            "epoch: 100  loss: 0.5652521237009007  params: [-0.37061145  0.49811669 -0.21089234]  gradients: [0.003961575326149342, -0.001168809371467594, 0.003972184884128214]\n",
            "epoch: 110  loss: 0.5549808324168746  params: [-0.40871317  0.51055666 -0.24939222]  gradients: [0.0036896705206105385, -0.0013023232444610042, 0.003753705395127248]\n",
            "epoch: 120  loss: 0.5457500831040304  params: [-0.44421658  0.52421924 -0.28583811]  gradients: [0.003439483487454707, -0.0014157247245364345, 0.0035586380977908004]\n",
            "epoch: 130  loss: 0.537399398679006  params: [-0.47733011  0.53891592 -0.3204506 ]  gradients: [0.003209416361487732, -0.0015112972171089927, 0.0033845293715757977]\n",
            "epoch: 140  loss: 0.5297964335929984  params: [-0.50824617  0.55448047 -0.35342617]  gradients: [0.002997878085436504, -0.0015912070260486993, 0.0032290181586752662]\n",
            "epoch: 150  loss: 0.5228320615121572  params: [-0.53714156  0.57076737 -0.38493857]  gradients: [0.002803336203082617, -0.0016574487158887254, 0.003089908515700281]\n",
            "epoch: 160  loss: 0.5164161681967029  params: [-0.56417822  0.58765001 -0.41514067]  gradients: [0.002624346359137233, -0.0017118211424026785, 0.002965204664326686]\n",
            "epoch: 170  loss: 0.5104741391368729  params: [-0.58950419  0.60501865 -0.44416653]  gradients: [0.0024595667157843704, -0.0017559232628225297, 0.0028531215143845884]\n",
            "epoch: 180  loss: 0.5049439747296929  params: [-0.61325472  0.62277847 -0.47213351]  gradients: [0.0023077626097143217, -0.0017911618299669874, 0.0027520801537923153]\n",
            "epoch: 190  loss: 0.4997739448279108  params: [-0.63555324  0.64084775 -0.49914425]  gradients: [0.002167805188407272, -0.0018187655638207065, 0.002660694859483976]\n",
            "epoch: 200  loss: 0.49492069297212643  params: [-0.6565125   0.65915612 -0.52528852]  gradients: [0.002038666553512645, -0.0018398022737630114, 0.002577755943221036]\n",
            "epoch: 210  loss: 0.4903477084157691  params: [-0.67623548  0.67764312 -0.55064488]  gradients: [0.0019194130665875016, -0.00185519673898306, 0.002502211145913135]\n",
            "epoch: 220  loss: 0.48602409539112434  params: [-0.6948163   0.69625682 -0.57528214]  gradients: [0.0018091978681069772, -0.0018657480603357466, 0.00243314720196412]\n",
            "epoch: 230  loss: 0.481923580895195  params: [-0.71234106  0.71495265 -0.59926065]  gradients: [0.0017072532541536599, -0.0018721457894664588, 0.002369772476440224]\n",
            "epoch: 240  loss: 0.4780237131569063  params: [-0.72888858  0.73369244 -0.62263347]  gradients: [0.00161288328816976, -0.0018749845154832079, 0.0023114011202533887]\n",
            "epoch: 250  loss: 0.47430521233126105  params: [-0.74453108  0.75244346 -0.64544727]  gradients: [0.0015254568534979445, -0.0018747768171437679, 0.00225743890723617]\n",
            "epoch: 260  loss: 0.4707514427595351  params: [-0.75933481  0.77117775 -0.66774327]  gradients: [0.0014444012445342584, -0.0018719646202035429, 0.0022073707514283732]\n",
            "epoch: 270  loss: 0.46734798245920767  params: [-0.77336057  0.78987139 -0.68955788]  gradients: [0.0013691963282884251, -0.0018669290700383146, 0.002160749811501863]\n",
            "epoch: 280  loss: 0.4640822705660784  params: [-0.78666423  0.808504   -0.71092341]  gradients: [0.0012993692691968487, -0.0018599990621517319, 0.0021171880441918514]\n",
            "epoch: 290  loss: 0.46094331746135564  params: [-0.79929717  0.82705824 -0.73186856]  gradients: [0.0012344897883269908, -0.0018514585828858581, 0.002076348051654032]\n",
            "epoch: 300  loss: 0.45792146547930807  params: [-0.81130668  0.84551938 -0.75241888]  gradients: [0.0011741659171968285, -0.001841553009306673, 0.002037936067110535]\n",
            "epoch: 310  loss: 0.4550081905790691  params: [-0.82273633  0.86387496 -0.77259722]  gradients: [0.0011180402021261412, -0.0018304945069494653, 0.002001695931588098]\n",
            "epoch: 320  loss: 0.4521959373201189  params: [-0.83362626  0.88211448 -0.79242405]  gradients: [0.0010657863146528819, -0.0018184666506344245, 0.0019674039274181834]\n",
            "epoch: 330  loss: 0.44947798101957204  params: [-0.84401355  0.90022915 -0.81191775]  gradients: [0.0010171060253859501, -0.0018056283791340172, 0.0019348643487086437]\n",
            "epoch: 340  loss: 0.4468483121818762  params: [-0.85393241  0.91821163 -0.83109489]  gradients: [0.0009717265016611553, -0.0017921173803628185, 0.001903905703614679]\n",
            "epoch: 350  loss: 0.44430153924919474  params: [-0.86341445  0.93605586 -0.84997041]  gradients: [0.0009293978928669777, -0.0017780529906389559, 0.0018743774570606634]\n",
            "epoch: 360  loss: 0.4418328064794512  params: [-0.8724889   0.95375689 -0.8685579 ]  gradients: [0.0008898911709208583, -0.0017635386797407496, 0.0018461472351669925]\n",
            "epoch: 370  loss: 0.4394377243620365  params: [-0.88118282  0.97131073 -0.88686968]  gradients: [0.0008529961968816673, -0.0017486641830421, 0.001819098423856307]\n",
            "epoch: 380  loss: 0.437112310462166  params: [-0.88952124  0.98871421 -0.90491697]  gradients: [0.0008185199879600257, -0.0017335073329226363, 0.0017931281039442696]\n",
            "epoch: 390  loss: 0.4348529389699841  params: [-0.89752737  1.00596489 -0.92271007]  gradients: [0.0007862851621802466, -0.001718135633816657, 0.0017681452735386531]\n",
            "epoch: 400  loss: 0.4326562975399701  params: [-0.90522271  1.02306095 -0.94025839]  gradients: [0.000756128540638798, -0.001702607618561536, 0.0017440693158941594]\n",
            "epoch: 410  loss: 0.43051935025589244  params: [-0.91262721  1.04000111 -0.95757059]  gradients: [0.0007278998896999091, -0.001686974017997464, 0.0017208286771325063]\n",
            "epoch: 420  loss: 0.4284393057587067  params: [-0.91975937  1.05678456 -0.97465469]  gradients: [0.0007014607875869622, -0.001671278770924849, 0.0016983597235708191]\n",
            "epoch: 430  loss: 0.4264135897391814  params: [-0.92663638  1.0734109  -0.99151807]  gradients: [0.00067668360169191, -0.0016555598974223084, 0.0016766057529324532]\n",
            "epoch: 440  loss: 0.42443982113109824  params: [-0.93327421  1.08988008 -1.00816761]  gradients: [0.0006534505645599328, -0.0016398502550581486, 0.0016555161375569525]\n",
            "epoch: 450  loss: 0.42251579145069657  params: [-0.93968768  1.10619234 -1.02460969]  gradients: [0.0006316529379381229, -0.0016241781945959422, 0.00163504558098151]\n",
            "epoch: 460  loss: 0.4206394468182555  params: [-0.94589061  1.12234821 -1.04085027]  gradients: [0.0006111902555292876, -0.0016085681293175294, 0.0016151534720234369]\n",
            "epoch: 470  loss: 0.41880887227209573  params: [-0.95189581  1.13834842 -1.05689495]  gradients: [0.0005919696361870872, -0.0015930410299934638, 0.0015958033228281514]\n",
            "epoch: 480  loss: 0.41702227804684855  params: [-0.95771523  1.15419389 -1.07274895]  gradients: [0.000573905160246481, -0.0015776148557612205, 0.0015769622793251484]\n",
            "epoch: 490  loss: 0.41527798753890116  params: [-0.96335999  1.16988573 -1.0884172 ]  gradients: [0.0005569173025215652, -0.001562304929674153, 0.0015586006942111931]\n",
            "epoch: 500  loss: 0.4135744267244557  params: [-0.96884045  1.18542518 -1.10390435]  gradients: [0.000540932416236802, -0.0015471242664161877, 0.0015406917540023543]\n",
            "epoch: 510  loss: 0.4119101148311316  params: [-0.97416625  1.20081358 -1.11921478]  gradients: [0.0005258822628010013, -0.0015320838586021035, 0.0015232111529043268]\n",
            "epoch: 520  loss: 0.41028365609376  params: [-0.9793464   1.21605239 -1.13435265]  gradients: [0.000511703582897968, -0.0015171929271707441, 0.0015061368072772404]\n",
            "epoch: 530  loss: 0.4086937324499798  params: [-0.98438927  1.23114315 -1.14932192]  gradients: [0.0004983377048638903, -0.0015024591406026468, 0.001489448605345049]\n",
            "epoch: 540  loss: 0.4071390970522203  params: [-0.9893027   1.24608747 -1.16412635]  gradients: [0.00048573018675791634, -0.0014878888070333422, 0.001473128187544297]\n",
            "epoch: 550  loss: 0.4056185684904037  params: [-0.99409399  1.26088701 -1.17876952]  gradients: [0.0004738304889169825, -0.0014734870427706717, 0.0014571587535425355]\n",
            "epoch: 560  loss: 0.40413102563466263  params: [-0.99876996  1.27554347 -1.19325485]  gradients: [0.00046259167412522693, -0.0014592579202439202, 0.0014415248924996517]\n",
            "epoch: 570  loss: 0.4026754030200829  params: [-1.00333696  1.29005861 -1.20758562]  gradients: [0.00045197013282810593, -0.0014452045980016322, 0.001426212433610089]\n",
            "epoch: 580  loss: 0.4012506867062718  params: [-1.00780096  1.3044342  -1.22176497]  gradients: [0.00044192533108656476, -0.0014313294350232102, 0.0014112083143620242]\n",
            "epoch: 590  loss: 0.3998559105537407  params: [-1.0121675   1.31867202 -1.23579592]  gradients: [0.0004324195792017353, -0.001417634091307555, 0.0013965004642913419]\n",
            "epoch: 600  loss: 0.3984901528669335  params: [-1.01644179  1.33277388 -1.24968137]  gradients: [0.0004234178191491775, -0.00140411961644287, 0.001382077702301793]\n",
            "epoch: 610  loss: 0.39715253336042367  params: [-1.02062867  1.3467416  -1.26342411]  gradients: [0.0004148874291470874, -0.0013907865276387298, 0.0013679296458755162]\n",
            "epoch: 620  loss: 0.3958422104105509  params: [-1.0247327   1.36057698 -1.27702684]  gradients: [0.00040679804384785096, -0.001377634878509466, 0.0013540466307157995]\n",
            "epoch: 630  loss: 0.39455837855968584  params: [-1.02875813  1.37428184 -1.29049215]  gradients: [0.00039912138878946536, -0.001364664319732123, 0.0013404196395519826]\n",
            "epoch: 640  loss: 0.3933002662445429  params: [-1.03270894  1.38785799 -1.30382256]  gradients: [0.0003918311278744115, -0.0013518741525591, 0.0013270402389987362]\n",
            "epoch: 650  loss: 0.39206713372360663  params: [-1.03658886  1.40130722 -1.31702049]  gradients: [0.00038490272276103007, -0.0013392633760416087, 0.0013139005235026202]\n",
            "epoch: 660  loss: 0.39085827118187133  params: [-1.04040137  1.41463133 -1.33008832]  gradients: [0.00037831330315727264, -0.0013268307287127715, 0.0013009930655304956]\n",
            "epoch: 670  loss: 0.38967299699382146  params: [-1.04414976  1.42783208 -1.34302832]  gradients: [0.00037204154710084026, -0.0013145747253859583, 0.0012883108712601558]\n",
            "epoch: 680  loss: 0.38851065612790453  params: [-1.04783708  1.44091124 -1.3558427 ]  gradients: [0.0003660675703940743, -0.001302493689643095, 0.0012758473411252538]\n",
            "epoch: 690  loss: 0.3873706186778088  params: [-1.05146621  1.45387054 -1.36853362]  gradients: [0.0003603728244378, -0.0012905857825171855, 0.0012635962346464768]\n",
            "epoch: 700  loss: 0.3862522785076022  params: [-1.05503985  1.46671171 -1.38110317]  gradients: [0.0003549400017765529, -0.001278849027812056, 0.0012515516390504587]\n",
            "epoch: 710  loss: 0.3851550519993294  params: [-1.05856052  1.47943644 -1.39355338]  gradients: [0.0003497529487289775, -0.0012672813344487249, 0.0012397079412385457]\n",
            "epoch: 720  loss: 0.3840783768930057  params: [-1.06203061  1.49204641 -1.40588623]  gradients: [0.0003447965845327173, -0.0012558805161812293, 0.001228059802720395]\n",
            "epoch: 730  loss: 0.3830217112100951  params: [-1.06545233  1.50454328 -1.41810366]  gradients: [0.00034005682648295264, -0.0012446443089837954, 0.001216602137173687]\n",
            "epoch: 740  loss: 0.3819845322525916  params: [-1.06882779  1.51692868 -1.43020753]  gradients: [0.00033552052058910363, -0.0012335703863756132, 0.0012053300903315813]\n",
            "epoch: 750  loss: 0.3809663356706984  params: [-1.07215894  1.52920423 -1.44219969]  gradients: [0.0003311753773150239, -0.0012226563729181032, 0.001194239021935049]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.07413714,  1.53651749, -1.4493421 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YkqKk65_Itn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dce153d-0602-452f-c919-334e19218a03"
      },
      "source": [
        "new_param_sgd = gradient_descent(X_train, y_train,model='logistic',batch_size=1,learning_rate=0.01,num_epoch=100)\n",
        "new_param_sgd"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: 1.209076578247759  params: [0.32738684 0.53327362 0.33682388]  gradients: [0.007041614757412125, 0.003832053486063481, 0.004954414950564038]\n",
            "epoch: 10  loss: 0.28607501573025157  params: [-0.96217732  1.18921818 -1.12288632]  gradients: [0.0024963035447543615, 0.0013584907766900018, 0.0017563760627856751]\n",
            "epoch: 20  loss: 0.22520376425437483  params: [-1.14525539  1.8051963  -1.7242547 ]  gradients: [0.00202230246588611, 0.0011005389362029937, 0.0014228732920957014]\n",
            "epoch: 30  loss: 0.19317237325674452  params: [-1.259278    2.22238775 -2.12649101]  gradients: [0.0017611739810395796, 0.0009584325649884493, 0.0012391457077401424]\n",
            "epoch: 40  loss: 0.17188994292655302  params: [-1.34724604  2.53033027 -2.42113367]  gradients: [0.0015830561425681487, 0.0008615006669282681, 0.001113823645644072]\n",
            "epoch: 50  loss: 0.1566792078709699  params: [-1.41774935  2.77004103 -2.6491284 ]  gradients: [0.0014534481096938375, 0.0007909678514721206, 0.0010226326336521476]\n",
            "epoch: 60  loss: 0.1452907335468325  params: [-1.47551998  2.96317885 -2.83195667]  gradients: [0.0013551326737437888, 0.0007374644971925272, 0.0009534588031426869]\n",
            "epoch: 70  loss: 0.13646642672445403  params: [-1.52368611  3.12255136 -2.98224067]  gradients: [0.001278194088086361, 0.000695594445288453, 0.0008993255265877364]\n",
            "epoch: 80  loss: 0.12944669095361022  params: [-1.56440608  3.25637135 -3.10802701]  gradients: [0.001216512166200466, 0.0006620270844013591, 0.0008559266974130326]\n",
            "epoch: 90  loss: 0.12374600766897836  params: [-1.59921731  3.37022891 -3.21476325]  gradients: [0.0011661075189351054, 0.0006345968271491133, 0.0008204624542539556]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.62643373,  3.4589472 , -3.29775225])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFFnGadF_Ito",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7a78e6-5115-4410-d3ad-f3d41c218256"
      },
      "source": [
        "new_param_mgd = gradient_descent(X_train, y_train, tolerance=0.00001,batch_size=20, learning_rate=0.005)\n",
        "new_param_mgd"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: 0.5407989462641625  params: [0.73968116 0.54219055 0.27430011]  gradients: [0.0007803989092934484, 0.0008946742516384953, 0.0012485159238316019]\n",
            "epoch: 10  loss: 0.4981692744521261  params: [0.60308788 0.53202522 0.19055053]  gradients: [0.0007212344793348868, 0.0008293290254336154, 0.0011836929782021188]\n",
            "epoch: 20  loss: 0.46123941735269575  params: [0.47579072 0.52476971 0.11133424]  gradients: [0.0006590838310858836, 0.0007604397478824564, 0.0011156036640499508]\n",
            "epoch: 30  loss: 0.429806999974738  params: [0.35780987 0.5207709  0.03705029]  gradients: [0.0005957300968159206, 0.0006902058701228185, 0.0010465096821390826]\n",
            "epoch: 40  loss: 0.4034550153484412  params: [ 0.24898638  0.52018916 -0.03212292]  gradients: [0.0005329744865997197, 0.0006208673579689957, 0.000978651232867915]\n",
            "epoch: 50  loss: 0.3816184565780065  params: [ 0.14899489  0.52300339 -0.09622378]  gradients: [0.0004723901169293921, 0.0005543791212167051, 0.0009139296289809509]\n",
            "epoch: 60  loss: 0.36366249009205587  params: [ 0.05737631  0.52904135 -0.15547252]  gradients: [0.00041516428704288445, 0.0004921951577665296, 0.0008537108967991485]\n",
            "epoch: 70  loss: 0.3489517449877117  params: [-0.0264193   0.53802464 -0.21021793]  gradients: [0.0003620520973372392, 0.0004352020512744375, 0.000798780957494756]\n",
            "epoch: 80  loss: 0.3368988875639038  params: [-0.10298981  0.54961541 -0.26088122]  gradients: [0.00031341819213129827, 0.00038377460991720114, 0.0007494183469489311]\n",
            "epoch: 90  loss: 0.32699066013364614  params: [-0.17294493  0.56345588 -0.30790825]  gradients: [0.0002693240900692874, 0.00033789572956455374, 0.0007055241166025353]\n",
            "epoch: 100  loss: 0.3187959551105529  params: [-0.23687993  0.57919666 -0.35173514]  gradients: [0.00022962400101384032, 0.0002972881899074231, 0.000666757478161579]\n",
            "epoch: 110  loss: 0.31196243831026316  params: [-0.29535873  0.59651413 -0.39276716]  gradients: [0.00019404754251616823, 0.00026152758030488255, 0.0006326485084982621]\n",
            "epoch: 120  loss: 0.30620737731880354  params: [-0.34890463  0.61511905 -0.43136814]  gradients: [0.0001622617220114298, 0.00023012553458293334, 0.0006026790902649392]\n",
            "epoch: 130  loss: 0.3013065269042984  params: [-0.39799654  0.63475927 -0.46785715]  gradients: [0.00013391286989616666, 0.0002025844804835829, 0.0005763346616358844]\n",
            "epoch: 140  loss: 0.29708325639161587  params: [-0.44306868  0.6552187  -0.50250971]  gradients: [0.00010865260577745053, 0.00017842998647646727, 0.0005531336149307033]\n",
            "epoch: 150  loss: 0.2933989330160005  params: [-0.48451241  0.67631434 -0.53556124]  gradients: [8.615243142678897e-05, 0.00015722744710427867, 0.0005326414202528878]\n",
            "epoch: 160  loss: 0.29014488056635857  params: [-0.52267918  0.6978925  -0.56721167]  gradients: [6.611082356519122e-05, 0.00013858871981952168, 0.0005144751815722177]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.52577334,  0.69960392, -0.56883988])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stsoveat_Ito"
      },
      "source": [
        "### Predict Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D17JMoUA_Ito"
      },
      "source": [
        "y_predict = []\n",
        "for i in range(len(y_test)):\n",
        "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
        "    if p> 0.5 :\n",
        "        y_predict.append(1)\n",
        "    else :\n",
        "        y_predict.append(0)\n",
        "y_predict_random = []\n",
        "for i in range(len(y_test)):\n",
        "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
        "    if p> 0.5 :\n",
        "        y_predict_random.append(1)\n",
        "    else :\n",
        "        y_predict_random.append(0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_KyFbDV_Ito"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcngOThr_Itp"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fj37Exj_Itp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd3d925-fee7-43b7-b1b8-dcfaadb0a1ca"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
        "confusion_matrix(y_test, y_predict)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[38,  2],\n",
              "       [ 7,  3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h62XYwH_Itq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60829366-da9b-41c4-e13c-97c279e6e81e"
      },
      "source": [
        "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
        "print(\"accuracy:\",accuracy)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGWeF92e_Itq"
      },
      "source": [
        "## Linear regression\n",
        "### $y = 0.5 + 2.7x$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bk6xJD8_Itq"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiEWLPB8_Itq"
      },
      "source": [
        "raw_X = np.random.rand(150)\n",
        "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J9TqvKZ_Itr"
      },
      "source": [
        "tmp = np.array([1 for _ in range(150)])\n",
        "X = np.vstack((tmp, raw_X)).T\n",
        "X = pd.DataFrame(X)\n",
        "y = pd.Series(y)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HUEi5rF_Itr"
      },
      "source": [
        "### Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_kfTiAq_Itr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d8bf4d-a454-4d52-a113-bb156a22daa8"
      },
      "source": [
        "#정규방정식\n",
        "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
        "theta"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.70248902, 2.33584725])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V665yRIp_Itr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fe23b6-d915-4ddc-e1e8-b9b74c3a91bc"
      },
      "source": [
        "#경사하강법\n",
        "new_param = gradient_descent(X, y,model=\"linear\")\n",
        "new_param"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  loss: 0.20491919352484342  params: [1.02603685 0.81430297]  gradients: [-0.03022111683065594, -0.02549747090428872]\n",
            "epoch: 10  loss: 0.06040508288704041  params: [1.06050895 1.60123398]  gradients: [-0.011032146484742277, -0.011017519275860818]\n",
            "epoch: 20  loss: 0.04527287726303129  params: [0.89229927 1.9322449 ]  gradients: [-0.010171332318745622, -0.009066169673177407]\n",
            "epoch: 30  loss: 0.03915981255570532  params: [0.80134374 2.11122956]  gradients: [-0.009705910077772965, -0.008011055370125293]\n",
            "epoch: 40  loss: 0.03645931711465966  params: [0.75216204 2.20801059]  gradients: [-0.009454245808288513, -0.0074405313447369024]\n",
            "epoch: 50  loss: 0.035175981908658366  params: [0.72556839 2.26034227]  gradients: [-0.009318165279683576, -0.007132036177283384]\n",
            "epoch: 60  loss: 0.03453377101858252  params: [0.7111886  2.28863918]  gradients: [-0.009244583478407842, -0.006965225904433953]\n",
            "epoch: 70  loss: 0.034201634299929104  params: [0.70341313 2.30393996]  gradients: [-0.009204796144139802, -0.006875027839557352]\n",
            "epoch: 80  loss: 0.034026461642292435  params: [0.69920876 2.31221343]  gradients: [-0.009183282237973971, -0.006826255717981654]\n",
            "epoch: 90  loss: 0.03393303450250858  params: [0.69693536 2.31668709]  gradients: [-0.00917164918521921, -0.00679988353531623]\n",
            "epoch: 100  loss: 0.033882894294909746  params: [0.69570608 2.31910609]  gradients: [-0.009165358932052161, -0.006785623503230909]\n",
            "epoch: 110  loss: 0.03385589286193123  params: [0.69504138 2.3204141 ]  gradients: [-0.009161957650742809, -0.006777912783278221]\n",
            "epoch: 120  loss: 0.03384132488946571  params: [0.69468197 2.32112137]  gradients: [-0.009160118501468804, -0.006773743423641909]\n",
            "epoch: 130  loss: 0.03383345710587783  params: [0.69448762 2.32150381]  gradients: [-0.009159124032107308, -0.006771488957148066]\n",
            "epoch: 140  loss: 0.03382920558042225  params: [0.69438253 2.3217106 ]  gradients: [-0.009158586300165153, -0.006770269916428307]\n",
            "epoch: 150  loss: 0.0338269074925842  params: [0.69432571 2.32182242]  gradients: [-0.009158295536414382, -0.006769610753703814]\n",
            "epoch: 160  loss: 0.03382566510093838  params: [0.69429499 2.32188288]  gradients: [-0.009158138313917175, -0.006769254329598913]\n",
            "epoch: 170  loss: 0.03382499338087878  params: [0.69427837 2.32191558]  gradients: [-0.009158053300178147, -0.006769061603072008]\n",
            "epoch: 180  loss: 0.03382463018692519  params: [0.69426939 2.32193325]  gradients: [-0.009158007331338118, -0.006768957391505898]\n",
            "epoch: 190  loss: 0.03382443380569049  params: [0.69426453 2.32194281]  gradients: [-0.0091579824749528, -0.006768901041969136]\n",
            "epoch: 200  loss: 0.03382432761966722  params: [0.6942619  2.32194798]  gradients: [-0.009157969034546432, -0.0067688705725077285]\n",
            "epoch: 210  loss: 0.03382427020300163  params: [0.69426048 2.32195078]  gradients: [-0.00915796176701653, -0.006768854096984908]\n",
            "epoch: 220  loss: 0.033824239156667116  params: [0.69425972 2.32195229]  gradients: [-0.009157957837299705, -0.0067688451882992945]\n",
            "epoch: 230  loss: 0.03382422236925925  params: [0.6942593 2.3219531]  gradients: [-0.009157955712413483, -0.0067688403711727105]\n",
            "epoch: 240  loss: 0.03382421329194293  params: [0.69425908 2.32195355]  gradients: [-0.009157954563439713, -0.006768837766444042]\n",
            "epoch: 250  loss: 0.033824208383637616  params: [0.69425895 2.32195379]  gradients: [-0.00915795394216378, -0.006768836358008704]\n",
            "epoch: 260  loss: 0.0338242057296077  params: [0.69425889 2.32195391]  gradients: [-0.00915795360622591, -0.006768835596436054]\n",
            "epoch: 270  loss: 0.03382420429451437  params: [0.69425885 2.32195398]  gradients: [-0.009157953424576757, -0.0067688351846366054]\n",
            "epoch: 280  loss: 0.033824203518527184  params: [0.69425883 2.32195402]  gradients: [-0.009157953326354967, -0.006768834961967372]\n",
            "epoch: 290  loss: 0.033824203098933504  params: [0.69425882 2.32195404]  gradients: [-0.009157953273244241, -0.006768834841565117]\n",
            "epoch: 300  loss: 0.03382420287204984  params: [0.69425882 2.32195405]  gradients: [-0.009157953244526089, -0.0067688347764609195]\n",
            "epoch: 310  loss: 0.03382420274936868  params: [0.69425882 2.32195406]  gradients: [-0.00915795322899752, -0.006768834741257604]\n",
            "epoch: 320  loss: 0.03382420268303224  params: [0.69425881 2.32195406]  gradients: [-0.009157953220600883, -0.006768834722222382]\n",
            "epoch: 330  loss: 0.03382420264716261  params: [0.69425881 2.32195406]  gradients: [-0.009157953216060624, -0.006768834711929609]\n",
            "epoch: 340  loss: 0.03382420262776713  params: [0.69425881 2.32195407]  gradients: [-0.00915795321360561, -0.0067688347063640745]\n",
            "epoch: 350  loss: 0.03382420261727954  params: [0.69425881 2.32195407]  gradients: [-0.009157953212278122, -0.006768834703354662]\n",
            "epoch: 360  loss: 0.03382420261160865  params: [0.69425881 2.32195407]  gradients: [-0.00915795321156033, -0.00676883470172741]\n",
            "epoch: 370  loss: 0.033824202608542284  params: [0.69425881 2.32195407]  gradients: [-0.009157953211172201, -0.00676883470084752]\n",
            "epoch: 380  loss: 0.033824202606884214  params: [0.69425881 2.32195407]  gradients: [-0.009157953210962327, -0.0067688347003717385]\n",
            "epoch: 390  loss: 0.03382420260598769  params: [0.69425881 2.32195407]  gradients: [-0.009157953210848852, -0.006768834700114479]\n",
            "epoch: 400  loss: 0.03382420260550292  params: [0.69425881 2.32195407]  gradients: [-0.00915795321078749, -0.006768834699975371]\n",
            "epoch: 410  loss: 0.0338242026052408  params: [0.69425881 2.32195407]  gradients: [-0.009157953210754308, -0.0067688346999001505]\n",
            "epoch: 420  loss: 0.03382420260509908  params: [0.69425881 2.32195407]  gradients: [-0.00915795321073637, -0.006768834699859484]\n",
            "epoch: 430  loss: 0.0338242026050224  params: [0.69425881 2.32195407]  gradients: [-0.009157953210726667, -0.006768834699837487]\n",
            "epoch: 440  loss: 0.033824202604980924  params: [0.69425881 2.32195407]  gradients: [-0.00915795321072141, -0.006768834699825588]\n",
            "epoch: 450  loss: 0.03382420260495857  params: [0.69425881 2.32195407]  gradients: [-0.009157953210718581, -0.006768834699819164]\n",
            "epoch: 460  loss: 0.0338242026049464  params: [0.69425881 2.32195407]  gradients: [-0.009157953210717043, -0.0067688346998156824]\n",
            "epoch: 470  loss: 0.033824202604939874  params: [0.69425881 2.32195407]  gradients: [-0.009157953210716212, -0.006768834699813803]\n",
            "epoch: 480  loss: 0.03382420260493638  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715783, -0.006768834699812804]\n",
            "epoch: 490  loss: 0.033824202604934434  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715533, -0.006768834699812244]\n",
            "epoch: 500  loss: 0.033824202604933365  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715395, -0.006768834699811945]\n",
            "epoch: 510  loss: 0.03382420260493278  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715315, -0.0067688346998117715]\n",
            "epoch: 520  loss: 0.03382420260493249  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715287, -0.006768834699811691]\n",
            "epoch: 530  loss: 0.03382420260493238  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715273, -0.006768834699811654]\n",
            "epoch: 540  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715256, -0.006768834699811627]\n",
            "epoch: 550  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 560  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 570  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 580  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 590  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 600  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 610  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 620  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 630  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 640  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 650  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 660  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 670  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 680  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 690  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 700  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 710  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 720  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 730  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 740  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 750  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 760  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 770  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 780  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 790  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 800  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 810  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 820  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 830  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 840  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 850  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 860  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 870  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 880  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 890  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 900  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 910  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 920  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 930  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 940  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 950  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 960  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 970  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 980  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n",
            "epoch: 990  loss: 0.03382420260493227  params: [0.69425881 2.32195407]  gradients: [-0.009157953210715263, -0.006768834699811617]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.69425881, 2.32195407])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ5H81IR_Itr"
      },
      "source": [
        "y_hat_NE = theta.dot(X.T)\n",
        "y_hat_GD = new_param.dot(X.T)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbxJeEk_Its"
      },
      "source": [
        "### Visualization\n",
        "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
        "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nECMOOOJ_Its",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "cb657a35-fc1c-4044-eb12-f7cb9f79f9ba"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
        "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
        "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD5CAYAAAA6JL6mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xVVf3/8deaGyBCEaD5FRHN/P3y9kOcX0VefiRe8n7Jr9W3xPuEAomh+EVFyVEwL18MsRoyMyO1vKNpWSQiMWaQd41UMh1TQUwS0QFmPr8/zgwNcO5n39Y+7+fjcR4wM/vss9be+3z22p+11t7OzBAREX/VxF0AERGpjAK5iIjnFMhFRDynQC4i4jkFchERzymQi4h4ri6IlTjnXgXeBzqADWbWmG/5QYMG2bBhw4L4aBGRqrF06dJ3zGzw5r8PJJB3+aKZvVPMgsOGDWPJkiUBfrSISPo55/6e7fdKrYiIeC6oQG7Aw865pc65poDWKSIiRQgqtbKfmb3hnNsG+K1z7i9mtrDnAl0Bvglg6NChAX2siIgEEsjN7I2uf1c45+4BPgss3GyZOcAcgMbGxi1u8LJ+/Xra2tr46KOPgihSbHr37s2QIUOor6+PuygiUiUqDuTOub5AjZm93/X/Q4DLSl1PW1sb/fr1Y9iwYTjnKi1WLMyMVatW0dbWxk477RR3cUSkSgSRI98WWOScexp4AviVmf261JV89NFHDBw40NsgDuCcY+DAgd5fVYiIXypukZvZcuD/BFAWr4N4tzTUQfJrbW1lwYIFjBo1ipEjR8ZdHJFAx5GLpF5rayujR49m3bp1NDQ0MH/+fAVziZ3GkffgnGPSpEkbf77mmmuYNm0aANOmTWP77bdn+PDhG1/vvfdeTCWVuCxYsIB169bR0dHBunXrWLBgQUnvb21tZcaMGbS2toZTQKlKapH30KtXL+6++26mTJnCoEGDtvj7ueeey3nnnRdDySQpRo0aRUNDw8YW+ahRo4p+r1rzEha1yHuoq6ujqamJmTNnxl0USaiRI0cyf/58mpubSw7ElbbmRXJJZIt84kR46qlg1zl8OFx3XeHlxo0bx1577cXkyZO3+NvMmTOZO3cuAAMGDOCRRx4JtpDihZEjR5bVkq6kNS+STyIDeZz69+/PmDFjmDVrFn369Nnkb0qtSCW6W/Ma8SJBS2QgL6blHKaJEycyYsQITj311HgLIqlTbmteJB/lyLP4xCc+wYknnsiPf/zjuIsiIlKQAnkOkyZN4p13Nr29+syZMzcZfvjqq6/GUzgRkR4SmVqJy5o1azb+f9ttt2Xt2rUbf542bdrGMeUiIkmiFrmIiOcUyEUkMfLNfPV5VmzYZVdqRUQSId/MV59nxUZRdrXIRSQR8s189XlWbBRlVyAXkUTonvlaW1u7xczXfH9LuijKrtSKiCRCvpmvSZ0VW8y96aMouzPb4vGZoWtsbLQlS5Zs8rsXX3yRz3zmM5GXpae3336bc889l8cff5wBAwbQ0NDA5MmTGTBgAMcccww777wza9euZdttt2Xy5MkceeSRWdeThLqISLjiyNs755aaWePmv1dqpYuZceyxx3LAAQewfPlyli5dyu23305bWxsA+++/P08++STLli1j1qxZjB8/nvnz58dcahGJS5Ly9grkXX7/+9/T0NDA2LFjN/5uxx13ZMKECVssO3z4cC655BJmz54dZRFFJEGSlLdPZo48hvvYPv/884wYMaLo1Y0YMYKrr746iJKJiIeSlLdPZiBPgHHjxrFo0SIaGhqyBuw4+hZEJFmScjfLZAbyGO5ju/vuu3PXXXdt/PmGG27gnXfeobFxi34FAJ588kl1aIpIIihH3uXAAw/ko48+4gc/+MHG3/W8aVZPzzzzDM3NzYwbNy6q4omI5JTMFnkMnHPce++9nHvuuVx11VUMHjyYvn378t3vfheAxx57jL333pu1a9eyzTbbMGvWLEaPHh1zqUWKG8ucJtVW32IEFsidc7XAEuANM8s+wDrhtttuO26//fasf1u9enXEpREpzOd7kJSj2upbrCBTK+cALwa4PhEpIEljmaNQbfUtViCB3Dk3BDgCuDGI9YlIcZI0ljkK1VbfYgWVWrkOmAz0y7WAc64JaAIYOnRo1mXMDOdcQEWKh4YlSpSSNJY5SLny4Gmtb6UqvteKc+5I4HAzO9s5Nwo4r1COPNu9Vv72t7/Rr18/Bg4c6G0wNzNWrVrF+++/z0477RR3cUS8pDx4brnutRJEi3xf4Gjn3OFAb6C/c26umX2jlJUMGTKEtrY2Vq5cGUCR4tO7d2+GDBkSdzECpVECEqVseXAdd/lVHMjNbAowBaBHi7ykIA5QX1+vVmwCqXUkUevOg3cfc77lweNo+GgcueSl1pFUqtTA5nMePK6GT6CB3MwWAAuCXKfEy/fWkcSr3MCWlHuYlCquho+m6Ete3a2j5uZmpVWkZD6N+w7iSfdxDY9UakUK8rV1JPEbOHAgNTU1mFmir+iCSonElRZSIBeRULS2tjJx4kQ6OjqoqanhuuuuS2yDIMiUSBwNH6VWRCQU3cGxs7Nz4xyLpPJ9xqha5CISCp86yn0eKQMBzOwsR7aZnSKSPsUOPfRp0lmcZQ1zZqeISFbF5It9mnSW1LIqRy4isfJpiGJSy6pALiKx8qmjMallVWpFRGLlU0djUsuqzk4REU/k6uxUakVExHMK5CISuiDuYyK5KUcuIqFK6pC9NFGLXEKhFph0S+qQvTRRi1wCpxaY9BTEVH2fZn7GQYFcAqenCklPlQ7ZU8OgMAVyCVwYN0tSi8xvldzaNU0Ng7COYwVyCVzQkyaKaZEp0KeXT3dRzCfMKwsFcglFkDfXL9QiC/vSWyeJeCV1NmWpwryyUCBPgbQHmkItsjC/IMrPJkMaHjcY5pWFArnnqiHQFGqRhfkF8T0/m/aTvE/CvLJQIPec74GmWPlaZGF+QXzOz1bDSd43YV1ZVBzInXO9gYVAr6713Wlml1a6XimOz4EmSGF9QXzOz1bLSV6CaZG3Awea2RrnXD2wyDn3kJk9HsC6pQCfA00Ywkgl+Jqf1Um+elQcyC1zH9w1XT/Wd72ivzduFfM10AStlFRCNeSOdZJPjo4OuOMOePNNGD8e6uuDXX8gOXLnXC2wFNgFuMHM/hjEeqtBNQSUqBSbSqim3LFO8vFob4ef/hQuvxzeeX0tTczhIq5gMO/w9//7d3bcb2ignxdIIDezDmC4c+7jwD3OuT3M7LmeyzjnmoAmgKFDg62Er6opoESh2FSCcsfBq/YGyQcfwA9/CM3NULP6Xb7FLC7mcpro2GLZHfu/BwQcA80s0BdwCXBevmX22WcfE7Pp06dbbW2tAVZbW2vTp0+Pu0jeW7x4sU2fPt0WL16cd5k+ffpYbW2t9enTJ++yUlg1bs9Vq8wuvdSspsbsP2izmZxjBrlfX/ua2TPPVPy5wBLLElODGLUyGFhvZu855/oABwPfrXS91UCdUcErJpWg3HGwquEK56WX4Ljj4PnnYSSL+W+uZBr3My3XG846CyZPhmHDIilfEKmV7YCfduXJa4BfmtkDAaw39RRQ4qPccXDS2CBZuhSOOALefhuOYh7zOIbn8r1hyhSYOBG22SaqIm5CD18WkYr5kiPPVc758+HQQ6GjwziTHzGHb+Zf0UknwQ03QL9+IZd4U7kevqyZnRIaX77cUjkfrnB6Di6orT2RdetGUkMHF3EFl3EpG/K9uakJZs6ErbaKqrglUSBPkSQFTo3IkaQwy4woOfvskfRmFbOYxNkdPwBuy/2mSy6BqVOhzo8Q6UcppaCkBc5q6ACTZOrogBkzMnF4AO9yI2dwFvdwVr43ff/7MHYsOBdVMQOlhy+nRLbAGafuDrDa2trUdIBJMrW3Z/oZnYOh7jX+WPcFLp7qMBzvMpDjuWeL9yybPn3TAYJnnVV2EE/Cg8bV2ZkS2VrkQKypliSleiQ93n8/E3d//nPYg2e5j2PYmb/lfsPWW8ODD8L++wdelqivhNXZmXKbD2UEYk+1+NABJvkl4WS8YgWMGQO/+Q0cwKM8xGHM5UPm5nrDLrvAvffC7rtv8usw6pKYFGK2WUJhvzSzM3zVMGu0mFmcaRVF3eOasbl8udk++2RyHifwy/wzJsFs//3NXn89lrpEvY0Ia2anJFMaJ2n0lLTO3ShFVfdcrc2gW7bPPgtHHw2vvmpM4HpmcQ55E69f+UpmGMrHP15xXSqVlEl9CuQplZQDLCyJuaSNQVR1z9YYCOIksmgRHHYYfLRmPZdxCVO4Ml+GG845B668Enr3DrQuQUlCClGBPMWScICFJe1XHPlEVfdsjYEZM2aUfBJ54AE46ijoyxquZwKncjPv53vDjBlw/vlQWxtqXcIWZf+CRq30kISOHcmv5z6CeEflxCnIY7WUdRVqkZvBLbfAKafAYFbwU07mMH6dvwA/+QmcfLK3Y7h76t6WAwcOZOLEiYGnv3KNWlFnZ5dqvBWnb7SPglfONu3Z0drRYXbNNZk+x5152f7M8MKdkw88EEHNotdzW9bV1VlNTU3ggw3I0dmpCUFdkjahRrakfRS8Urfp+vUwb95ILrxwCuO/0MDbtdsx6bzM5JtX2IW9eWrTNwweDI8/vmkoP+KI8CoUo57bsrOzk9ra2sgmxCmQd6lkJmISZnZVg0pni1bTfiq2roW26dq1cOaZmazHIe5h6hscM67MBO6lNLIdb226wj33hGXL/h20V6ygtbOzKrZ7z23Zq1cvZs+eTXNzczQjqrI108N+JTG1Ylbe2Nw0XO77NB673LKmYT8Vq9S69tymq1aZHXtsJgp/g1sKp0kOPtjszTdzrrulpcXq6+utpqYm9dvdLPzvEjlSKwrkFfJ94k21BDjf91MpSqlrW5vZfvuZOTrsPK4qHLhPOsls9eqiyrF48WKrq6szwACrqalJ9XaPQq5ArtRKhXy/OVS15J192E9BpX7y1XXZMthtN+jl2rnWTWL7IY7HFjk6qeVqJm+5svPPz9yVqjuU33IL9O9fVDkWLFhAZ2fnxp9ra2sTsd1TmWLLFt3DfqWpRW6W3NSEHkS8qaTuJ7Pg90N3XW+88WkbPNisH6ttLv9VuMV97bVmHR1Fr7+YY6umpsbq6uqspaWl5PIHva98P95RaqW6lHLAJjnAVYugUj+//a2Zc2af5B/2Ow4sHLjnzi35M8I+tsIMtr6n2HIFcs3sTKlSpnGnZQaozxO6yp2t+ctfZm49sivLuJvjOYgX6My1cF0dPPQQHHRQRWUN+9gK8xYEqZ0RnC26h/1Sizx8vl9ClioN9S3Ueu3sNJs9O9OQ/jyLbRUD8re2hwwx+/OfQylnmNs6ivX7egWKUivJF/QB5vMBWyofLplL3R8bNph95zuZb+mRzCucJmlszNwDNsQyVfq+MNdfDcd71QRyX3dmGlqUccq2/ZJ0LBSzfz/80Gz8eDPotDNpKRy4jzrKbOXKUMvkizTVJZ9cgTxVww+7b+gzdepURo8e7dXwomoZBhiW7rvbdc+kAxJ1LGTbv//6F3z961DrOrjEXUbvPo7rZzuMGubwzS1X0tQEa9b8O5TPmweDBgVaJl+lqS7lqDiQO+d2cM494px7wTn3vHPunCAKVg6fd6YP45yTbuTIkUyZMoWRI0cm7lgYNWoU9fVDgIep73ifj134Ov0/5vj5rY4O6riMS7d80yWXZG5u0h24W1qgb99Ay5SWYy6KuiR6/Hm2ZnopL2A7YETX//sBfwV2y/eesFIrvl9eJSkV4LskHAuvvGK2995mA1hld3Fc4VTJ97+f6dGMUJqOuTDrkoTjySzCHDlwH3BwvmWUI5coRH0sPP202dChZjvwd/sDIwsG7uMT3DErm0pKZ3okgRwYBrwG9M/ytyZgCbBk6NCh0dRaYlEtJ9OFC8222spsD56xV9gpf+DeemuzRx81s+S07qpV0iYplSL0QA5sDSwFji+0rIYfpldSDvgw3Hdf5htzAAvsA/rkD9y77GL27LM511UtJ7ukqeT4TMI+yxXIA5nZ6ZyrB+4Cfm5mdwexziTxecZg1NLyUGQzuPlmOO00+DJ3cif/ydFkbuOX1X77wa23wg47FLX+tMym9U0lx2eS91nFgdw554AfAy+a2f9UXqRkCeKp4dXE1ynQnZ1w7bUwebIxjhuYzQROBU7N9YYTToA5c2DAgAhLKZXy9fgsJIhx5PsCJwEHOuee6nodHsB6A1HpkKGkDWNLus3Hcyf1pLd+PVxwAdS79VzuLqam1nH+5MwY7tlM2PIN48fDhx/+O3lyxx0K4h7y5fgsWbZ8S9ivqHLkQeRr05zzrSZr1piddprZVqyxGzmt4IgSu+KKzBx5kQShGmZ2bi6I1nRqz+Apt2oVHHMMDHYredAdTt+tHT++yfEBW3M6N235hptuyuRXukP5hRdCbW30BU+RRE+gqUAi65Utuof98qlFnkRJ6D1PmtdfN9t3X7Odedn+zPDCLe7774+7yKmW5u9enPWiGu9H3t2aTtOIk2I7X9M+0uYvf4Fjj4W+y5byAEcyhLdYlGvhQYPggQfgc5+LsohVLS2jlzaX1HqlOpBDsocMlaOYAymNI22eeAIOOwz2efdhHuZQ/jfwl1wL77kn3Hkn7LprhCWUntI6OiSp9Up9IE+bYg6kcloNSWvBP/wwHHoofJ25zOUkPgusyrXwwQdnHgr8yU9GWELJJ41Xw5DgemXLt4T90szOyhTKkZeax4s772dmdtttZo4OO4+rCue3TzrJbPXqij4viQ9GECmEtD1YIg1flLDv1lbsuqO+IVBnp9n115vV027X8O3Cgfv8883a2wP7/FwnrlL3R67lk3BilHTKFci9TK34mgPumb4ACtahknRH9/LdQy7zvT/svN+GDdDcDDMv+xc/4Cy+zq2MB8bnesO118LEiVBT3ujYQtst17DUUo6pfMdgUjvEJMWyRfewX5W2yOO+pWQQd08bO3Zs3jpU2qorJ70S1NXBhx+ajRtn9kn+Yb/jwMIt7rlzK/7MbsXUO9sypR5T+ZZXi1zCQppa5HH2HJd7NbB5Kw3IW4dKW3Wlvr+S0T2rV8M3vwlP/mIZd3M8u/MCs4HZWZa12lrcQw9lOihDUEy9c3VYlXJM5TsGo+4QS1pHtcQgW3QP++Vzjrzcq4FSHw4cdYu8FG+9ZXbQQWafo9Xe4RP5W9vbb2+2dGlgn11IlLcpTUI/TVC3oYi7HsXyqaxhIG2dnXHxKVAEddC//LLZ8OFmR3B/4TTJPvtknnEWk8WLF9vYsWNt7NixOU+QaQoElaYZ40oD+fxwhzgpkAcormAQ1efefPNT9rH+79lp3Fg4cB9xhNmKFaGWp1iFvuhpDASV1imO/qZyyxx331gS5ArkXubI4xbWbNFcuc7W1lZuueUWbrrpJjo6OgIfqbNwIRx2SAcT26/kCi7mZODkXAufcQZcd12gT3MPSqH8eBpHk1Saj4+jv6nc/ZDUWZWJkC26h/2K8qZZvlxGd7dSampqrL6+3lpaWjb5vXPOyDygpuLWyD33mPVmrc3m7IIt7se++EWz9etLqkdc27waW+RBiHqfRZmeTBuqLbUS1Jc2qgNn+vTpVlNTszFY19XVbTEsDjDnXEn16ew0+9GPzD7Ou3YnxxcM3K9MmmR9evcu+0sWd6AsZtZrNQeCpAhyP1TTPq26QB5EPi3KwLR48WKrr6/fGLBramo2HpzdZejVq1fOTrxuHR1mM2aYDeE1W8QXCgZuu+OOrGUp54sRdQ6zmr7AcUvqtk5C4yFKVRfIg9jBUQemlpYWq6urs5qamqKnjre3m513ntkePGOvsFPeoN3Zt6/Zo4+GVv6oT3zV9AWOU5K3dbV1gOYK5Knt7AxiUkbUnStNTU3sueeeW5S5Z+fqmjUwYQIsv/lRHuIwtuJDrgauzrI++9SncPfeC3vsAYALtfTRToRJY8dlNkmY7JPkba0O0C7ZonvYL5+GH8Z9SblypdmRR5p9mTsKp0n23dfstddiKadPHWa+SEodk1KOXOL+jkaJakut+GrlSrOLLuy0Q3nIHmPf/IH7hBPM3n037iJ7NanEJ0lKG8S9reP+/KRQIE+o1183+9a4DfYVbrNn2T1/4B4/3mzt2riLvMWXKkkBJ02S3hIOWq5g7eN2COvEkyuQpzZHnlQvvQRXNbdT+7OfcDGXM4Q3+F6W5dbtsTcN37k482DKHrdzjTtnmu2mYQMHDqSmpgYzq+48ZZGK3YeJfRpNCNJ0W+BYbrOdLbqX+gJuAlYAzxWzfDW1yJ980uwbR6+2C5hh79M3Z2u7fb8vms2fnxn4nUMSWiabt77Hjh27cSJTXV3dxolMkl0S9mESpem2wGFeoZKjRV7enfu3dDPwpYDW5bVFi+DEUSu40v03OMfwvR0/m/cxrmQKW/PBxuVW7DuKn4wbR+vixWBGw2O/hwMPBJd7bEmuByJEqXuUQG1tLQ0NDQCsW7eOzs5OzIxVq3I+WVPYdB+2t7czbdo0Wltb4y5W7DY/rrLdFri5udmLh8jkq0toskX3cl7AMKqsRd7ZafarX5kdv/dy+yFNefPb68ecZvbXv5pZ8S2MzfNsSWmZ9CxXUsqUdN3brKWlZeMVDF0Tv7TdMtLUoRl1jlyBvAQbNpjdfrvZcZ962m7nxLyBe8O5k8zeeCPreoq59MoVIJN4sCexTEmy+b5saWmxQw45ZGMwD/LyW1Pf0y32QA40AUuAJUOHDo2k0pVqb8/cp+SEbR6133Bw7qBd38s6Lms2++c/i1pvMa1YX0aC6MteWLZ9GcaVTJDr1JVWMuUK5JGNWjGzOcAcgMbGRovqc0vxwQfQ8kNjyaX3M/GDy/ksf+IM4IzNlmsfsC0N37kYd8bp0KcPtWV81sknZ24UO2bMmKw5Px9mrPn6EOyoZduXYYxICXJ0h28jRapdVQ8//Oc/4fqZG1h++a1caJezKy/x7SzLfTj0f9Hniovhq1+Fujp6VfCZmwe/MWPGZF3Oh6Fn+rIXJ9e+DPq+9kGe/H1oSEgP2Zrppb6A24A3gfVAG3B6vuXLzZFXehn/5ptmF3xrrU3ge/Y2g3OmSj7Y63Nm99+fdyhguXxJmRRDl9/Joxx5uuH7zM5ygsby5WYTT37XpvIda6c+d+De/1CzhQtLLlM58tXDxy+Oj2UW8VWuQO5NaqWYy/jnn4cbLvoHu9x3Dd9mJjsBM7Os68Ojv0Kf5gthr70A2Cr84m+U6zLb13xzWI+9E5HieRPIs+XsnngCbrzgJT6/YAan8RN2B76f5b3tp3yTXlMnw847A9An0pJvKVvwiyLfHOT0/rhvFVAJn8suklW2ZnrYr3Jz5L/+9RN21H/cb3dxXM40iYG1f/sCs7fe2uS9SU8BhJ1v1tC0DJ/LLoLvqRWAB7/yAvNWn7LJ79b36oubejF1E86G/v0BaNjsfT6kLQqNUqm0FamhaRk+l10kF68C+YxF+7Py1MMYeOrR1Jx2CvTuTX0R7/Ply5sr3xzEiSio4WStra289tpr1NVlDh3fhqZpWJ2kkVeBfKs9dmarPz1Y8vt8//IGcSIKYlx6zxNKbW0tZ555Zs4JTZUKK4/tw/h8kVJ5FcjL5fuXN6gTUaUjTHqeUACGDh0aWhAPMxWmkTaSNlURyMHvL29STkRRXdn4kgoTSYqqCeS+S8KJKKoTiu+pMJGoucyIlmg1NjbakiVLIv/cNKiWMdDVUk+RUjjnlppZ4+a/V4vcI5Xkjn0LjEm4AhHxhQK5R8rNHfswjl5EyhfUMzslAuU+CzDbCUBE0kMt8h5KST9ElarY/HPK6WxU56FIuimQdykl/RBVqiLX58QxGajY8vqUhxdJCwXyLqXkn6Ma5xzk54Tdeag8vEh8lCPvUkr+udxcdZhlipvy8CLxUYu8Synph6hSFUmZ0VkM5eFF4qMJQV3Kze8qL/xv2hYi4dKEoDzKze9GnRdOeqDUJB6ReChHTvn53Sjzwt0njalTpzJ69GhaW1tD+ywR8YsCOeV3KkbZGanORBHJRakVyu9UjLIzUp2JIpKLOjs9kvQcuYiEK9TOTufcl4DvAbXAjWZ2ZRDrjYovAVKdiSKSTcWB3DlXC9wAHAy0AX9yzs0zsxcqXXcUNCNRcvHlBC8SRIv8s8DLZrYcwDl3O3AM4EUgr6bHiikwFU8nePFJEIF8e+D1Hj+3AZ8LYL2RqJZORAWm0lTTCV78F9nwQ+dck3NuiXNuycqVK0P7nNbWVmbMmFH0OOvukSfNzc2pDm4avlgan+5zIxJEi/wNYIcePw/p+t0mzGwOMAcyo1YC+NwtlNvqDLITManpizivPJK6TfLx6T43IkEE8j8Bn3bO7UQmgH8V+K8A1luyuC+H40hfFBsk4wpMSU/p5Nt+GiUkvqg4kJvZBufceOA3ZIYf3mRmz1dcsjLEne+O+kRSapCMIzDFfXLNJ+knGZFiBTKO3MweBB4MYl2ViKPV2bNFF/WJJMlBslvcJ9d8fNh+IsVI3RT9KFud2Vp0UZ5IkhwkuyU51+zD9hMpRuoCeZSyteimTJkSWbBKcpDsKam5Zl+2n0ghCuQVSEKLLqlB0hfafpIGCuQVKKVF5+MQPBHxgwJ5hYpp0Wl0hIiESQ+WiIBmVYpImBTII6Dp3iISplSmVpKWj9boCBEJU+oCeVLz0RodISJhSV1qRfloEak2qQvkykdXn1JvXSySNqlLrSgfXV2SmkoTiVLqAjkoH11NdOMrkRSmVqS6KJUmktIWuVQPpdJEFMglBZRKk2qn1EoOGgkhIr5QizwLjYQQEZ+oRZ6FJhWJiE8UyLPQSAgR8YlSK1loJISI+ESBPAeNhBARXyi1IiLiOQVyERHPVRTInXP/6Zx73jnX6ZxrDKpQIiJSvEpb5M8BxwMLAyiLiIiUoaLOTjN7EcA5F0xpRESkZJHlyJ1zTc65Jc65JStXrozqY0VEUq9gi9w59zvgk1n+dJGZ3VfsB5nZHGAOQGNjoxVdQhERyatgIDezg6IoiIiIlEfDD0VEPFfp8MPjnHNtwEjgV8653wRTLBERKValo1buAe4JqCwiIlIGpdm4I2AAAARHSURBVFZERDynQJ4yerKRSPXR3Q9TRE82EqlOapGniJ5sJFKdFMhTRE82EqlOSq2kiJ5sJFKdFMhTRk82Eqk+Sq2IiHhOgVxExHMK5CIinlMgFxHxnAK5iIjnFMhFRDznzKJ/WI9zbiXw9yIWHQS8E3Jxkqpa6656V59qrXs59d7RzAZv/stYAnmxnHNLzKwx7nLEoVrrrnpXn2qte5D1VmpFRMRzCuQiIp5LeiCfE3cBYlStdVe9q0+11j2weic6Ry4iIoUlvUUuIiIFJCKQO+e+5Jxb5px72Tn331n+3ss594uuv//ROTcs+lIGr4h6f9s594Jz7hnn3Hzn3I5xlDMMhereY7kvO+fMOZeKUQ3F1Ns5d2LXfn/eOXdr1GUMSxHH+1Dn3CPOuSe7jvnD4yhn0JxzNznnVjjnnsvxd+ecm9W1XZ5xzo0o+UPMLNYXUAu8AuwMNABPA7tttszZwA+7/v9V4Bdxlzuien8R2Krr/2elod7F1r1ruX7AQuBxoDHucke0zz8NPAkM6Pp5m7jLHWHd5wBndf1/N+DVuMsdUN0PAEYAz+X4++HAQ4ADPg/8sdTPSEKL/LPAy2a23MzWAbcDx2y2zDHAT7v+fycw2jnnIixjGArW28weMbO1XT8+DgyJuIxhKWafAzQD3wU+irJwISqm3mcCN5jZPwHMbEXEZQxLMXU3oH/X/z8G/CPC8oXGzBYC7+ZZ5BjgFst4HPi4c267Uj4jCYF8e+D1Hj+3df0u6zJmtgFYDQyMpHThKabePZ1O5qydBgXr3nV5uYOZ/SrKgoWsmH2+K7Crc+4PzrnHnXNfiqx04Sqm7tOAbzjn2oAHgQnRFC12pcaCLegJQR5wzn0DaAT+X9xliYJzrgb4H+CUmIsShzoy6ZVRZK7AFjrn9jSz92ItVTS+BtxsZtc650YCP3PO7WFmnXEXLOmS0CJ/A9ihx89Dun6XdRnnXB2Zy65VkZQuPMXUG+fcQcBFwNFm1h5R2cJWqO79gD2ABc65V8nkDeeloMOzmH3eBswzs/Vm9jfgr2QCu++KqfvpwC8BzKwV6E3mfiRpV1QsyCcJgfxPwKedczs55xrIdGbO22yZecDJXf8/Afi9dfUSeKxgvZ1zewMtZIJ4WnKlUKDuZrbazAaZ2TAzG0amf+BoM1sST3EDU8yxfi+Z1jjOuUFkUi3LoyxkSIqp+2vAaADn3GfIBPKVkZYyHvOAMV2jVz4PrDazN0taQ9w9uj16bf9Kplf7oq7fXUbmywuZHXoH8DLwBLBz3GWOqN6/A94Gnup6zYu7zFHVfbNlF5CCUStF7nNHJq30AvAs8NW4yxxh3XcD/kBmRMtTwCFxlzmget8GvAmsJ3PFdTowFhjbY5/f0LVdni3nWNfMThERzyUhtSIiIhVQIBcR8ZwCuYiI5xTIRUQ8p0AuIuI5BXIREc8pkIuIeE6BXETEc/8ffpam08DBD1UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWSH3yDC_Its"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}
